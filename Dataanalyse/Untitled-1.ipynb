{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  yr  mnth  holiday  weekday  workingday  weathersit      temp  \\\n",
       "0       1   0     1        0        6           0           2  0.344167   \n",
       "1       1   0     1        0        0           0           2  0.363478   \n",
       "2       1   0     1        0        1           1           1  0.196364   \n",
       "3       1   0     1        0        2           1           1  0.200000   \n",
       "4       1   0     1        0        3           1           1  0.226957   \n",
       "\n",
       "      atemp       hum  windspeed  casual  registered   cnt  \n",
       "0  0.363625  0.805833   0.160446     331         654   985  \n",
       "1  0.353739  0.696087   0.248539     131         670   801  \n",
       "2  0.189405  0.437273   0.248309     120        1229  1349  \n",
       "3  0.212122  0.590435   0.160296     108        1454  1562  \n",
       "4  0.229270  0.436957   0.186900      82        1518  1600  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a dataset from internet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "filename = 'Bike-Sharing-Dataset.zip'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "os.remove(filename)\n",
    "\n",
    "data = pd.read_csv('day.csv').drop(columns=['instant', 'dteday'])\n",
    "print(data.shape)\n",
    "data.head()\n",
    "data.to_csv('night.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('night.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 731\n",
      "Example data point: tensor([1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 6.0000e+00, 0.0000e+00,\n",
      "        2.0000e+00, 3.4417e-01, 3.6362e-01, 8.0583e-01, 1.6045e-01, 3.3100e+02,\n",
      "        6.5400e+02, 9.8500e+02])\n",
      "Batch 0: torch.Size([32, 14])\n",
      "Batch 1: torch.Size([32, 14])\n",
      "Batch 2: torch.Size([32, 14])\n",
      "Batch 3: torch.Size([32, 14])\n",
      "Batch 4: torch.Size([32, 14])\n",
      "Batch 5: torch.Size([32, 14])\n",
      "Batch 6: torch.Size([32, 14])\n",
      "Batch 7: torch.Size([32, 14])\n",
      "Batch 8: torch.Size([32, 14])\n",
      "Batch 9: torch.Size([32, 14])\n",
      "Batch 10: torch.Size([32, 14])\n",
      "Batch 11: torch.Size([32, 14])\n",
      "Batch 12: torch.Size([32, 14])\n",
      "Batch 13: torch.Size([32, 14])\n",
      "Batch 14: torch.Size([32, 14])\n",
      "Batch 15: torch.Size([32, 14])\n",
      "Batch 16: torch.Size([32, 14])\n",
      "Batch 17: torch.Size([32, 14])\n",
      "Batch 18: torch.Size([32, 14])\n",
      "Batch 19: torch.Size([32, 14])\n",
      "Batch 20: torch.Size([32, 14])\n",
      "Batch 21: torch.Size([32, 14])\n",
      "Batch 22: torch.Size([27, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use IterableDataset to load the data\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "import pandas as pd\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "\n",
    "# data = pd.read_csv('day.csv')\n",
    "# print(data.shape)\n",
    "class BikeSharingDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file).drop(columns=['instant', 'dteday'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return torch.tensor(row.to_numpy(), dtype=torch.float32)      \n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in  self.data :\n",
    "            print(\"line:\",line)\n",
    "            yield line\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "dataset = BikeSharingDataset('day.csv')\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Example data point: {dataset[0]}\")  \n",
    "# print data in each batch\n",
    "from torch.utils.data import DataLoader\n",
    "train_ld = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "for idx, batch_data in enumerate(train_ld):\n",
    "    print(f\"Batch {idx}:\", batch_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14])\n",
      "torch.Size([27, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import csv\n",
    "\n",
    "class BikeSharingIterableDataset(IterableDataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        # 将读取的数据行转换为张量，这里假设所有数据均为浮点数\n",
    "        return torch.tensor([float(x) for x in line], dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 打开文件，逐行读取数据\n",
    "        file = open(self.csv_file, 'r')\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # 跳过标题行\n",
    "        return map(self.parse_line, reader)\n",
    "\n",
    "# 使用示例\n",
    "dataset = BikeSharingIterableDataset('night.csv')\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 4800, 8920, 17744, 13452) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# DataLoader with multiple workers\u001b[39;00m\n\u001b[0;32m     42\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\28968\\.conda\\envs\\syntheic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 4800, 8920, 17744, 13452) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import csv\n",
    "\n",
    "class BikeSharingIterableDataset(IterableDataset):\n",
    "    def __init__(self, csv_file, start_line=None, end_line=None):\n",
    "        self.csv_file = csv_file\n",
    "        self.start_line = start_line\n",
    "        self.end_line = end_line\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        return torch.tensor([float(x) for x in line], dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            start = self.start_line\n",
    "            end = self.end_line\n",
    "        else:\n",
    "            # Split workload\n",
    "            per_worker = int((self.end_line - self.start_line) / worker_info.num_workers)\n",
    "            worker_id = worker_info.id\n",
    "            start = self.start_line + worker_id * per_worker\n",
    "            end = start + per_worker if worker_id != worker_info.num_workers - 1 else self.end_line\n",
    "\n",
    "        # Read the file and yield batches\n",
    "        file = open(self.csv_file, 'r')\n",
    "        reader = csv.reader(file)\n",
    "        current_line = 0\n",
    "        for line in reader:\n",
    "            if current_line >= start and current_line < end:\n",
    "                yield self.parse_line(line)\n",
    "            current_line += 1\n",
    "            if current_line >= end:\n",
    "                break\n",
    "        file.close()\n",
    "\n",
    "# Create the dataset\n",
    "dataset = BikeSharingIterableDataset('night.csv', start_line=0, end_line=731)\n",
    "\n",
    "# DataLoader with multiple workers\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=False)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break  # Display only the first batch to limit output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 50)  # 从 num_features 到 50\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)  # 从 50 到 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "num_features = 13  # 假设我们有15个特征\n",
    "model = SimpleNet(num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1], Loss: 2.7902\n",
      "Epoch [1/50], Step [11], Loss: 165.5290\n",
      "Epoch [2/50], Step [1], Loss: 8.9038\n",
      "Epoch [2/50], Step [11], Loss: 5.1525\n",
      "Epoch [3/50], Step [1], Loss: 0.3842\n",
      "Epoch [3/50], Step [11], Loss: 4.4250\n",
      "Epoch [4/50], Step [1], Loss: 2.8230\n",
      "Epoch [4/50], Step [11], Loss: 3.8881\n",
      "Epoch [5/50], Step [1], Loss: 0.3350\n",
      "Epoch [5/50], Step [11], Loss: 3.3226\n",
      "Epoch [6/50], Step [1], Loss: 9.7008\n",
      "Epoch [6/50], Step [11], Loss: 1.3690\n",
      "Epoch [7/50], Step [1], Loss: 4.4954\n",
      "Epoch [7/50], Step [11], Loss: 0.7048\n",
      "Epoch [8/50], Step [1], Loss: 21.3020\n",
      "Epoch [8/50], Step [11], Loss: 1.5055\n",
      "Epoch [9/50], Step [1], Loss: 9.5335\n",
      "Epoch [9/50], Step [11], Loss: 45.8951\n",
      "Epoch [10/50], Step [1], Loss: 32.1552\n",
      "Epoch [10/50], Step [11], Loss: 327.5278\n",
      "Epoch [11/50], Step [1], Loss: 5.1768\n",
      "Epoch [11/50], Step [11], Loss: 547.6213\n",
      "Epoch [12/50], Step [1], Loss: 0.4834\n",
      "Epoch [12/50], Step [11], Loss: 63.5702\n",
      "Epoch [13/50], Step [1], Loss: 11.2810\n",
      "Epoch [13/50], Step [11], Loss: 13.8012\n",
      "Epoch [14/50], Step [1], Loss: 0.7170\n",
      "Epoch [14/50], Step [11], Loss: 13.9227\n",
      "Epoch [15/50], Step [1], Loss: 1.0364\n",
      "Epoch [15/50], Step [11], Loss: 5.8820\n",
      "Epoch [16/50], Step [1], Loss: 1.6661\n",
      "Epoch [16/50], Step [11], Loss: 0.9982\n",
      "Epoch [17/50], Step [1], Loss: 1.2623\n",
      "Epoch [17/50], Step [11], Loss: 0.5500\n",
      "Epoch [18/50], Step [1], Loss: 1.2574\n",
      "Epoch [18/50], Step [11], Loss: 0.2327\n",
      "Epoch [19/50], Step [1], Loss: 1.3317\n",
      "Epoch [19/50], Step [11], Loss: 0.2995\n",
      "Epoch [20/50], Step [1], Loss: 1.0678\n",
      "Epoch [20/50], Step [11], Loss: 0.3199\n",
      "Epoch [21/50], Step [1], Loss: 1.5011\n",
      "Epoch [21/50], Step [11], Loss: 0.4521\n",
      "Epoch [22/50], Step [1], Loss: 0.7867\n",
      "Epoch [22/50], Step [11], Loss: 0.7474\n",
      "Epoch [23/50], Step [1], Loss: 2.0650\n",
      "Epoch [23/50], Step [11], Loss: 1.0002\n",
      "Epoch [24/50], Step [1], Loss: 0.3714\n",
      "Epoch [24/50], Step [11], Loss: 1.6859\n",
      "Epoch [25/50], Step [1], Loss: 4.2253\n",
      "Epoch [25/50], Step [11], Loss: 1.8566\n",
      "Epoch [26/50], Step [1], Loss: 0.8897\n",
      "Epoch [26/50], Step [11], Loss: 2.6994\n",
      "Epoch [27/50], Step [1], Loss: 11.7398\n",
      "Epoch [27/50], Step [11], Loss: 3.7465\n",
      "Epoch [28/50], Step [1], Loss: 7.3081\n",
      "Epoch [28/50], Step [11], Loss: 13.7784\n",
      "Epoch [29/50], Step [1], Loss: 33.1493\n",
      "Epoch [29/50], Step [11], Loss: 84.1431\n",
      "Epoch [30/50], Step [1], Loss: 28.9044\n",
      "Epoch [30/50], Step [11], Loss: 509.2132\n",
      "Epoch [31/50], Step [1], Loss: 43.1250\n",
      "Epoch [31/50], Step [11], Loss: 1251.7461\n",
      "Epoch [32/50], Step [1], Loss: 2.8670\n",
      "Epoch [32/50], Step [11], Loss: 283.1419\n",
      "Epoch [33/50], Step [1], Loss: 7.6543\n",
      "Epoch [33/50], Step [11], Loss: 15.1134\n",
      "Epoch [34/50], Step [1], Loss: 3.9069\n",
      "Epoch [34/50], Step [11], Loss: 36.7219\n",
      "Epoch [35/50], Step [1], Loss: 2.7269\n",
      "Epoch [35/50], Step [11], Loss: 2.0019\n",
      "Epoch [36/50], Step [1], Loss: 0.4941\n",
      "Epoch [36/50], Step [11], Loss: 0.2630\n",
      "Epoch [37/50], Step [1], Loss: 1.2684\n",
      "Epoch [37/50], Step [11], Loss: 1.2948\n",
      "Epoch [38/50], Step [1], Loss: 1.1339\n",
      "Epoch [38/50], Step [11], Loss: 0.1636\n",
      "Epoch [39/50], Step [1], Loss: 0.9681\n",
      "Epoch [39/50], Step [11], Loss: 0.1576\n",
      "Epoch [40/50], Step [1], Loss: 1.0711\n",
      "Epoch [40/50], Step [11], Loss: 0.0907\n",
      "Epoch [41/50], Step [1], Loss: 0.9682\n",
      "Epoch [41/50], Step [11], Loss: 0.0615\n",
      "Epoch [42/50], Step [1], Loss: 0.9778\n",
      "Epoch [42/50], Step [11], Loss: 0.0865\n",
      "Epoch [43/50], Step [1], Loss: 0.9475\n",
      "Epoch [43/50], Step [11], Loss: 0.0585\n",
      "Epoch [44/50], Step [1], Loss: 0.9332\n",
      "Epoch [44/50], Step [11], Loss: 0.0717\n",
      "Epoch [45/50], Step [1], Loss: 0.9056\n",
      "Epoch [45/50], Step [11], Loss: 0.0549\n",
      "Epoch [46/50], Step [1], Loss: 0.8990\n",
      "Epoch [46/50], Step [11], Loss: 0.0636\n",
      "Epoch [47/50], Step [1], Loss: 0.8670\n",
      "Epoch [47/50], Step [11], Loss: 0.0515\n",
      "Epoch [48/50], Step [1], Loss: 0.8695\n",
      "Epoch [48/50], Step [11], Loss: 0.0598\n",
      "Epoch [49/50], Step [1], Loss: 0.8268\n",
      "Epoch [49/50], Step [11], Loss: 0.0496\n",
      "Epoch [50/50], Step [1], Loss: 0.8485\n",
      "Epoch [50/50], Step [11], Loss: 0.0599\n"
     ]
    }
   ],
   "source": [
    "# 模型、优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练过程\n",
    "def train_model(model, data_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            # 假设最后一列是目标值\n",
    "            inputs = data[:, :-1]\n",
    "            targets = data[:, -1]\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 数据加载器\n",
    "dataset = BikeSharingIterableDataset('night.csv')\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 开始训练\n",
    "train_model(model, loader, criterion, optimizer, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deutsch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
