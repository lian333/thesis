{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "iris = datasets.load_breast_cancer()\n",
    "X = iris.data  # we only take the first four features.\n",
    "y = iris.target\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "real_data = pd.DataFrame(X)\n",
    "real_labels = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_labels = one_hot_encoder.fit_transform(np.array(real_labels).reshape(-1, 1))\n",
    "\n",
    "NOISE_DIM = 100\n",
    "NUM_CLASSES = 2\n",
    "NUM_FEATURES = 30\n",
    "BATCH_SIZE = 64\n",
    "TRAINING_STEPS = 500\n",
    "# Constants\n",
    "# Generator\n",
    "def create_generator():\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([noise_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(NUM_FEATURES, activation='linear')(hidden)\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def create_discriminator():\n",
    "    data_input = Input(shape=(NUM_FEATURES,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([data_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[data_input, class_input], outputs=output)\n",
    "    return model\n",
    "# cGAN\n",
    "def create_cgan(generator, discriminator):\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    generated_data = generator([noise_input, class_input])\n",
    "    validity = discriminator([generated_data, class_input])\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=validity)\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = create_generator()\n",
    "# Create and compile the Discriminator\n",
    "discriminator = create_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "gan = create_cgan(generator, discriminator)\n",
    "\n",
    "# Ensure that only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "\n",
       "   class  \n",
       "0      0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAN\n",
    "for step in range(TRAINING_STEPS):\n",
    "    # Select a random batch of real data with labels\n",
    "    idx = np.random.randint(0, real_data.shape[0], BATCH_SIZE)\n",
    "    real_batch = real_data.iloc[idx].values\n",
    "    labels_batch = one_hot_labels[idx]\n",
    "\n",
    "    # Generate a batch of new data\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))\n",
    "    generated_batch = generator.predict([noise, labels_batch])\n",
    "\n",
    "    # Train the discriminator\n",
    "    real_loss = discriminator.train_on_batch([real_batch, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "    fake_loss = discriminator.train_on_batch([generated_batch, labels_batch], np.zeros((BATCH_SIZE, 1)))\n",
    "    discriminator_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "    # Train the generator\n",
    "    generator_loss = gan.train_on_batch([noise, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate instances for a given class\n",
    "def generate_data(generator, data_class, num_instances):\n",
    "    one_hot_class = one_hot_encoder.transform(np.array([[data_class]]))\n",
    "    noise = np.random.normal(0, 1, (num_instances, NOISE_DIM))\n",
    "    generated_data = generator.predict([noise, np.repeat(one_hot_class, num_instances, axis=0)])\n",
    "    return pd.DataFrame(generated_data,columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.386197</td>\n",
       "      <td>0.430119</td>\n",
       "      <td>0.386089</td>\n",
       "      <td>0.245277</td>\n",
       "      <td>0.234082</td>\n",
       "      <td>0.319339</td>\n",
       "      <td>0.050656</td>\n",
       "      <td>0.200825</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>0.287857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314550</td>\n",
       "      <td>0.276671</td>\n",
       "      <td>0.229163</td>\n",
       "      <td>0.188168</td>\n",
       "      <td>0.449392</td>\n",
       "      <td>0.319546</td>\n",
       "      <td>0.446428</td>\n",
       "      <td>0.504266</td>\n",
       "      <td>0.293903</td>\n",
       "      <td>0.366978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.369419</td>\n",
       "      <td>0.206298</td>\n",
       "      <td>0.352262</td>\n",
       "      <td>0.226191</td>\n",
       "      <td>0.240948</td>\n",
       "      <td>0.174220</td>\n",
       "      <td>0.039027</td>\n",
       "      <td>-0.108835</td>\n",
       "      <td>0.318523</td>\n",
       "      <td>0.151682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052576</td>\n",
       "      <td>0.273266</td>\n",
       "      <td>0.243587</td>\n",
       "      <td>0.089666</td>\n",
       "      <td>0.254069</td>\n",
       "      <td>0.121691</td>\n",
       "      <td>0.252544</td>\n",
       "      <td>0.340169</td>\n",
       "      <td>0.279351</td>\n",
       "      <td>0.122819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.329358</td>\n",
       "      <td>0.441639</td>\n",
       "      <td>0.310998</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>0.152576</td>\n",
       "      <td>0.256034</td>\n",
       "      <td>0.351475</td>\n",
       "      <td>0.297466</td>\n",
       "      <td>0.244187</td>\n",
       "      <td>0.452779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205266</td>\n",
       "      <td>0.361498</td>\n",
       "      <td>0.389768</td>\n",
       "      <td>0.294411</td>\n",
       "      <td>0.332537</td>\n",
       "      <td>0.185718</td>\n",
       "      <td>0.281220</td>\n",
       "      <td>0.533877</td>\n",
       "      <td>0.296112</td>\n",
       "      <td>0.085709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.565993</td>\n",
       "      <td>0.066067</td>\n",
       "      <td>0.350571</td>\n",
       "      <td>0.216517</td>\n",
       "      <td>0.105980</td>\n",
       "      <td>0.229419</td>\n",
       "      <td>0.096651</td>\n",
       "      <td>-0.050360</td>\n",
       "      <td>0.425980</td>\n",
       "      <td>0.322431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067845</td>\n",
       "      <td>0.194928</td>\n",
       "      <td>0.107569</td>\n",
       "      <td>0.262432</td>\n",
       "      <td>0.151166</td>\n",
       "      <td>0.201589</td>\n",
       "      <td>0.287575</td>\n",
       "      <td>0.658456</td>\n",
       "      <td>-0.045699</td>\n",
       "      <td>0.114189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.474208</td>\n",
       "      <td>0.283670</td>\n",
       "      <td>0.421432</td>\n",
       "      <td>0.250591</td>\n",
       "      <td>0.287740</td>\n",
       "      <td>0.293671</td>\n",
       "      <td>0.250811</td>\n",
       "      <td>0.390907</td>\n",
       "      <td>0.539080</td>\n",
       "      <td>0.227571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253736</td>\n",
       "      <td>0.233148</td>\n",
       "      <td>0.364838</td>\n",
       "      <td>0.314779</td>\n",
       "      <td>0.241509</td>\n",
       "      <td>0.051218</td>\n",
       "      <td>0.235337</td>\n",
       "      <td>0.520116</td>\n",
       "      <td>0.279817</td>\n",
       "      <td>0.231730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.468026</td>\n",
       "      <td>0.223539</td>\n",
       "      <td>0.122982</td>\n",
       "      <td>0.206050</td>\n",
       "      <td>0.380983</td>\n",
       "      <td>0.358458</td>\n",
       "      <td>0.310780</td>\n",
       "      <td>0.284803</td>\n",
       "      <td>0.272772</td>\n",
       "      <td>0.461433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147190</td>\n",
       "      <td>0.377790</td>\n",
       "      <td>0.269374</td>\n",
       "      <td>0.342860</td>\n",
       "      <td>0.234561</td>\n",
       "      <td>0.065906</td>\n",
       "      <td>0.140395</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.108154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.201679</td>\n",
       "      <td>0.381338</td>\n",
       "      <td>0.298234</td>\n",
       "      <td>0.213378</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.403945</td>\n",
       "      <td>0.118311</td>\n",
       "      <td>0.188690</td>\n",
       "      <td>0.530882</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.337153</td>\n",
       "      <td>0.319736</td>\n",
       "      <td>-0.059347</td>\n",
       "      <td>0.176994</td>\n",
       "      <td>0.364039</td>\n",
       "      <td>0.270082</td>\n",
       "      <td>0.308225</td>\n",
       "      <td>0.127975</td>\n",
       "      <td>0.185774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.315498</td>\n",
       "      <td>0.358282</td>\n",
       "      <td>0.249849</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>0.210207</td>\n",
       "      <td>0.282692</td>\n",
       "      <td>0.230639</td>\n",
       "      <td>0.143786</td>\n",
       "      <td>0.396897</td>\n",
       "      <td>0.335360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384013</td>\n",
       "      <td>0.240521</td>\n",
       "      <td>0.179763</td>\n",
       "      <td>0.223506</td>\n",
       "      <td>0.305219</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.235471</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.100528</td>\n",
       "      <td>0.276636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.321690</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>0.257058</td>\n",
       "      <td>0.207339</td>\n",
       "      <td>0.166509</td>\n",
       "      <td>0.149121</td>\n",
       "      <td>0.083959</td>\n",
       "      <td>0.054531</td>\n",
       "      <td>0.158812</td>\n",
       "      <td>0.320911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197660</td>\n",
       "      <td>0.114888</td>\n",
       "      <td>0.280592</td>\n",
       "      <td>0.062215</td>\n",
       "      <td>0.394140</td>\n",
       "      <td>0.191486</td>\n",
       "      <td>0.207845</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.054814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.242951</td>\n",
       "      <td>0.613820</td>\n",
       "      <td>0.551912</td>\n",
       "      <td>0.537407</td>\n",
       "      <td>0.394751</td>\n",
       "      <td>0.415202</td>\n",
       "      <td>0.280240</td>\n",
       "      <td>0.329586</td>\n",
       "      <td>0.237438</td>\n",
       "      <td>0.722003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298275</td>\n",
       "      <td>0.395009</td>\n",
       "      <td>0.335233</td>\n",
       "      <td>0.233243</td>\n",
       "      <td>0.506370</td>\n",
       "      <td>0.377747</td>\n",
       "      <td>0.574603</td>\n",
       "      <td>0.743009</td>\n",
       "      <td>0.386556</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.248624</td>\n",
       "      <td>0.320875</td>\n",
       "      <td>0.214544</td>\n",
       "      <td>-0.043706</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.266619</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.279938</td>\n",
       "      <td>0.334327</td>\n",
       "      <td>0.309187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197058</td>\n",
       "      <td>0.342753</td>\n",
       "      <td>0.217666</td>\n",
       "      <td>0.258519</td>\n",
       "      <td>0.299003</td>\n",
       "      <td>0.139554</td>\n",
       "      <td>0.101857</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.177158</td>\n",
       "      <td>0.194537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.415100</td>\n",
       "      <td>0.344164</td>\n",
       "      <td>0.253655</td>\n",
       "      <td>0.264736</td>\n",
       "      <td>0.353636</td>\n",
       "      <td>0.419368</td>\n",
       "      <td>0.267184</td>\n",
       "      <td>0.151145</td>\n",
       "      <td>0.437623</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163153</td>\n",
       "      <td>0.367720</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.141381</td>\n",
       "      <td>0.412040</td>\n",
       "      <td>0.119012</td>\n",
       "      <td>0.468953</td>\n",
       "      <td>0.196210</td>\n",
       "      <td>0.220770</td>\n",
       "      <td>0.244765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.260474</td>\n",
       "      <td>0.298087</td>\n",
       "      <td>0.417001</td>\n",
       "      <td>0.262082</td>\n",
       "      <td>0.267811</td>\n",
       "      <td>0.364704</td>\n",
       "      <td>0.263458</td>\n",
       "      <td>0.276311</td>\n",
       "      <td>0.404660</td>\n",
       "      <td>0.206999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522413</td>\n",
       "      <td>0.277560</td>\n",
       "      <td>0.355798</td>\n",
       "      <td>0.135159</td>\n",
       "      <td>0.114075</td>\n",
       "      <td>0.618796</td>\n",
       "      <td>0.502527</td>\n",
       "      <td>0.569695</td>\n",
       "      <td>0.108649</td>\n",
       "      <td>0.468425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.111177</td>\n",
       "      <td>0.300627</td>\n",
       "      <td>0.130324</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>0.320916</td>\n",
       "      <td>0.178315</td>\n",
       "      <td>-0.025656</td>\n",
       "      <td>0.132050</td>\n",
       "      <td>0.248182</td>\n",
       "      <td>0.270094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197470</td>\n",
       "      <td>0.182067</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.055345</td>\n",
       "      <td>0.285406</td>\n",
       "      <td>0.178283</td>\n",
       "      <td>0.315887</td>\n",
       "      <td>0.149277</td>\n",
       "      <td>0.170247</td>\n",
       "      <td>0.277363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.216181</td>\n",
       "      <td>0.229471</td>\n",
       "      <td>0.249125</td>\n",
       "      <td>0.129681</td>\n",
       "      <td>0.130509</td>\n",
       "      <td>0.222792</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.120956</td>\n",
       "      <td>0.254522</td>\n",
       "      <td>0.382204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052249</td>\n",
       "      <td>0.369519</td>\n",
       "      <td>0.159547</td>\n",
       "      <td>0.195806</td>\n",
       "      <td>0.472969</td>\n",
       "      <td>0.151230</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.293819</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.195601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.467237</td>\n",
       "      <td>0.262001</td>\n",
       "      <td>0.257967</td>\n",
       "      <td>0.351264</td>\n",
       "      <td>0.248651</td>\n",
       "      <td>0.436804</td>\n",
       "      <td>0.299434</td>\n",
       "      <td>0.135787</td>\n",
       "      <td>0.713189</td>\n",
       "      <td>0.332204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070565</td>\n",
       "      <td>0.262594</td>\n",
       "      <td>0.233038</td>\n",
       "      <td>0.108998</td>\n",
       "      <td>0.325547</td>\n",
       "      <td>0.295953</td>\n",
       "      <td>0.376409</td>\n",
       "      <td>0.475550</td>\n",
       "      <td>0.368765</td>\n",
       "      <td>0.331211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.204724</td>\n",
       "      <td>0.156506</td>\n",
       "      <td>0.167053</td>\n",
       "      <td>0.102885</td>\n",
       "      <td>0.115033</td>\n",
       "      <td>0.102750</td>\n",
       "      <td>0.121288</td>\n",
       "      <td>-0.056602</td>\n",
       "      <td>0.190979</td>\n",
       "      <td>0.172960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084964</td>\n",
       "      <td>0.292609</td>\n",
       "      <td>0.116040</td>\n",
       "      <td>-0.052918</td>\n",
       "      <td>0.185662</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.134715</td>\n",
       "      <td>0.198203</td>\n",
       "      <td>0.213967</td>\n",
       "      <td>0.096693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.205129</td>\n",
       "      <td>0.283421</td>\n",
       "      <td>0.377584</td>\n",
       "      <td>0.193089</td>\n",
       "      <td>0.276089</td>\n",
       "      <td>0.294265</td>\n",
       "      <td>0.053723</td>\n",
       "      <td>0.232653</td>\n",
       "      <td>0.385599</td>\n",
       "      <td>0.200287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128811</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>0.190810</td>\n",
       "      <td>0.361980</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.308206</td>\n",
       "      <td>0.483131</td>\n",
       "      <td>0.134870</td>\n",
       "      <td>0.271625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.145191</td>\n",
       "      <td>0.202475</td>\n",
       "      <td>0.296088</td>\n",
       "      <td>0.080199</td>\n",
       "      <td>0.304828</td>\n",
       "      <td>0.251444</td>\n",
       "      <td>0.279713</td>\n",
       "      <td>0.114896</td>\n",
       "      <td>0.331973</td>\n",
       "      <td>0.324658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348772</td>\n",
       "      <td>0.105050</td>\n",
       "      <td>0.304204</td>\n",
       "      <td>0.048081</td>\n",
       "      <td>0.381806</td>\n",
       "      <td>0.082847</td>\n",
       "      <td>0.096901</td>\n",
       "      <td>0.410349</td>\n",
       "      <td>0.023723</td>\n",
       "      <td>0.098133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.272093</td>\n",
       "      <td>0.360578</td>\n",
       "      <td>0.332876</td>\n",
       "      <td>0.150166</td>\n",
       "      <td>0.406546</td>\n",
       "      <td>0.223171</td>\n",
       "      <td>0.230316</td>\n",
       "      <td>0.111944</td>\n",
       "      <td>0.356213</td>\n",
       "      <td>0.324140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280611</td>\n",
       "      <td>0.268189</td>\n",
       "      <td>0.359625</td>\n",
       "      <td>0.111875</td>\n",
       "      <td>0.239295</td>\n",
       "      <td>0.134976</td>\n",
       "      <td>0.271283</td>\n",
       "      <td>0.468120</td>\n",
       "      <td>0.172372</td>\n",
       "      <td>0.249658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.237796</td>\n",
       "      <td>0.511094</td>\n",
       "      <td>0.256828</td>\n",
       "      <td>0.069686</td>\n",
       "      <td>0.155274</td>\n",
       "      <td>0.320165</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.118073</td>\n",
       "      <td>0.413859</td>\n",
       "      <td>0.378171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155993</td>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.168960</td>\n",
       "      <td>0.151737</td>\n",
       "      <td>0.046224</td>\n",
       "      <td>0.160137</td>\n",
       "      <td>0.186234</td>\n",
       "      <td>0.131438</td>\n",
       "      <td>-0.007395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.564727</td>\n",
       "      <td>0.533837</td>\n",
       "      <td>0.545933</td>\n",
       "      <td>0.376148</td>\n",
       "      <td>0.342575</td>\n",
       "      <td>0.348650</td>\n",
       "      <td>0.398068</td>\n",
       "      <td>0.456945</td>\n",
       "      <td>0.567430</td>\n",
       "      <td>0.311718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571587</td>\n",
       "      <td>0.610831</td>\n",
       "      <td>0.534961</td>\n",
       "      <td>0.440498</td>\n",
       "      <td>0.564862</td>\n",
       "      <td>0.183993</td>\n",
       "      <td>0.403239</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.333165</td>\n",
       "      <td>0.449125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.336237</td>\n",
       "      <td>0.103055</td>\n",
       "      <td>0.152478</td>\n",
       "      <td>0.365210</td>\n",
       "      <td>0.439378</td>\n",
       "      <td>0.332149</td>\n",
       "      <td>0.189341</td>\n",
       "      <td>0.396250</td>\n",
       "      <td>0.344556</td>\n",
       "      <td>0.302989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308640</td>\n",
       "      <td>0.370322</td>\n",
       "      <td>0.311813</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.276649</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>0.235396</td>\n",
       "      <td>0.496657</td>\n",
       "      <td>0.293125</td>\n",
       "      <td>0.148750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.178119</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>0.152957</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>0.250289</td>\n",
       "      <td>0.175728</td>\n",
       "      <td>0.168876</td>\n",
       "      <td>0.157699</td>\n",
       "      <td>0.140369</td>\n",
       "      <td>0.014595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254024</td>\n",
       "      <td>0.271049</td>\n",
       "      <td>0.279352</td>\n",
       "      <td>0.010098</td>\n",
       "      <td>0.310728</td>\n",
       "      <td>0.239477</td>\n",
       "      <td>0.222729</td>\n",
       "      <td>0.408675</td>\n",
       "      <td>0.143942</td>\n",
       "      <td>0.182837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.439003</td>\n",
       "      <td>0.581463</td>\n",
       "      <td>0.413993</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.256457</td>\n",
       "      <td>0.431809</td>\n",
       "      <td>0.352558</td>\n",
       "      <td>0.138457</td>\n",
       "      <td>0.460704</td>\n",
       "      <td>0.449049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137508</td>\n",
       "      <td>0.558278</td>\n",
       "      <td>-0.032345</td>\n",
       "      <td>-0.014140</td>\n",
       "      <td>0.624411</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.488920</td>\n",
       "      <td>0.384686</td>\n",
       "      <td>0.494755</td>\n",
       "      <td>0.376439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.318287</td>\n",
       "      <td>0.224534</td>\n",
       "      <td>0.164119</td>\n",
       "      <td>0.129373</td>\n",
       "      <td>0.152149</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.152377</td>\n",
       "      <td>0.147458</td>\n",
       "      <td>0.298457</td>\n",
       "      <td>0.275234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224574</td>\n",
       "      <td>0.182682</td>\n",
       "      <td>0.183465</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.259364</td>\n",
       "      <td>0.135196</td>\n",
       "      <td>0.184464</td>\n",
       "      <td>0.222704</td>\n",
       "      <td>0.191220</td>\n",
       "      <td>0.256525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.599791</td>\n",
       "      <td>0.370321</td>\n",
       "      <td>0.645501</td>\n",
       "      <td>0.323689</td>\n",
       "      <td>0.684305</td>\n",
       "      <td>0.601636</td>\n",
       "      <td>0.703182</td>\n",
       "      <td>0.191165</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.526681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188982</td>\n",
       "      <td>0.705023</td>\n",
       "      <td>0.257867</td>\n",
       "      <td>0.234057</td>\n",
       "      <td>0.665222</td>\n",
       "      <td>0.154083</td>\n",
       "      <td>0.331064</td>\n",
       "      <td>0.568246</td>\n",
       "      <td>0.182178</td>\n",
       "      <td>0.134998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.426911</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.335961</td>\n",
       "      <td>0.206706</td>\n",
       "      <td>0.578716</td>\n",
       "      <td>0.468398</td>\n",
       "      <td>0.239256</td>\n",
       "      <td>0.385035</td>\n",
       "      <td>0.527718</td>\n",
       "      <td>0.477764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452361</td>\n",
       "      <td>0.286508</td>\n",
       "      <td>0.395223</td>\n",
       "      <td>0.264869</td>\n",
       "      <td>0.533061</td>\n",
       "      <td>0.276267</td>\n",
       "      <td>0.642933</td>\n",
       "      <td>0.481264</td>\n",
       "      <td>0.319794</td>\n",
       "      <td>0.413560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.172309</td>\n",
       "      <td>0.390668</td>\n",
       "      <td>0.273558</td>\n",
       "      <td>0.156306</td>\n",
       "      <td>0.448302</td>\n",
       "      <td>0.273477</td>\n",
       "      <td>0.229244</td>\n",
       "      <td>0.201789</td>\n",
       "      <td>0.497279</td>\n",
       "      <td>0.369095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034141</td>\n",
       "      <td>0.222412</td>\n",
       "      <td>0.194965</td>\n",
       "      <td>0.046520</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.160810</td>\n",
       "      <td>0.153244</td>\n",
       "      <td>0.467519</td>\n",
       "      <td>0.242481</td>\n",
       "      <td>0.294280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.358740</td>\n",
       "      <td>0.132950</td>\n",
       "      <td>0.165707</td>\n",
       "      <td>0.145584</td>\n",
       "      <td>0.472086</td>\n",
       "      <td>0.362978</td>\n",
       "      <td>0.288241</td>\n",
       "      <td>0.264629</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125465</td>\n",
       "      <td>0.146489</td>\n",
       "      <td>0.368904</td>\n",
       "      <td>-0.110773</td>\n",
       "      <td>0.598762</td>\n",
       "      <td>0.043465</td>\n",
       "      <td>0.142770</td>\n",
       "      <td>0.503588</td>\n",
       "      <td>0.377347</td>\n",
       "      <td>0.203947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.625863</td>\n",
       "      <td>0.252566</td>\n",
       "      <td>0.396583</td>\n",
       "      <td>0.405398</td>\n",
       "      <td>0.157031</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>0.169056</td>\n",
       "      <td>0.274989</td>\n",
       "      <td>0.143623</td>\n",
       "      <td>0.442094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155846</td>\n",
       "      <td>0.452356</td>\n",
       "      <td>0.132736</td>\n",
       "      <td>0.218936</td>\n",
       "      <td>0.337666</td>\n",
       "      <td>0.159508</td>\n",
       "      <td>0.296501</td>\n",
       "      <td>0.605024</td>\n",
       "      <td>0.524569</td>\n",
       "      <td>0.279083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.556992</td>\n",
       "      <td>0.446144</td>\n",
       "      <td>0.358348</td>\n",
       "      <td>0.275166</td>\n",
       "      <td>0.503915</td>\n",
       "      <td>0.515874</td>\n",
       "      <td>0.177182</td>\n",
       "      <td>0.419661</td>\n",
       "      <td>0.506074</td>\n",
       "      <td>0.344232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357109</td>\n",
       "      <td>0.582543</td>\n",
       "      <td>0.410344</td>\n",
       "      <td>0.396981</td>\n",
       "      <td>0.442774</td>\n",
       "      <td>0.332416</td>\n",
       "      <td>0.691754</td>\n",
       "      <td>0.561902</td>\n",
       "      <td>0.436272</td>\n",
       "      <td>0.354933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.442738</td>\n",
       "      <td>0.346256</td>\n",
       "      <td>0.396414</td>\n",
       "      <td>0.360849</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>0.326547</td>\n",
       "      <td>0.209490</td>\n",
       "      <td>0.245551</td>\n",
       "      <td>0.236719</td>\n",
       "      <td>0.219817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246329</td>\n",
       "      <td>0.421096</td>\n",
       "      <td>0.497765</td>\n",
       "      <td>0.233890</td>\n",
       "      <td>0.341040</td>\n",
       "      <td>0.231478</td>\n",
       "      <td>0.436393</td>\n",
       "      <td>0.329454</td>\n",
       "      <td>0.404347</td>\n",
       "      <td>0.309656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.630434</td>\n",
       "      <td>0.417026</td>\n",
       "      <td>0.368123</td>\n",
       "      <td>0.283383</td>\n",
       "      <td>0.364684</td>\n",
       "      <td>0.385365</td>\n",
       "      <td>0.136257</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.561446</td>\n",
       "      <td>0.049611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499782</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.314338</td>\n",
       "      <td>0.242399</td>\n",
       "      <td>0.358231</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>0.558835</td>\n",
       "      <td>0.567085</td>\n",
       "      <td>0.282957</td>\n",
       "      <td>0.424783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.545591</td>\n",
       "      <td>0.287892</td>\n",
       "      <td>0.271717</td>\n",
       "      <td>0.364813</td>\n",
       "      <td>0.394602</td>\n",
       "      <td>0.500491</td>\n",
       "      <td>0.184913</td>\n",
       "      <td>0.406670</td>\n",
       "      <td>0.743301</td>\n",
       "      <td>0.261611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428071</td>\n",
       "      <td>0.250202</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.144250</td>\n",
       "      <td>0.586456</td>\n",
       "      <td>0.320601</td>\n",
       "      <td>0.470379</td>\n",
       "      <td>0.405429</td>\n",
       "      <td>0.262271</td>\n",
       "      <td>0.362671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.336157</td>\n",
       "      <td>0.290638</td>\n",
       "      <td>0.363282</td>\n",
       "      <td>0.180508</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>0.266652</td>\n",
       "      <td>0.421781</td>\n",
       "      <td>0.283812</td>\n",
       "      <td>0.303687</td>\n",
       "      <td>0.151520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340252</td>\n",
       "      <td>0.389241</td>\n",
       "      <td>0.490092</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.349001</td>\n",
       "      <td>0.132289</td>\n",
       "      <td>0.202058</td>\n",
       "      <td>0.208451</td>\n",
       "      <td>0.277488</td>\n",
       "      <td>0.241132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.370611</td>\n",
       "      <td>0.357193</td>\n",
       "      <td>0.273274</td>\n",
       "      <td>0.477726</td>\n",
       "      <td>0.081189</td>\n",
       "      <td>0.290937</td>\n",
       "      <td>0.057073</td>\n",
       "      <td>0.261683</td>\n",
       "      <td>0.479012</td>\n",
       "      <td>0.337940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269190</td>\n",
       "      <td>0.197426</td>\n",
       "      <td>0.241530</td>\n",
       "      <td>0.190675</td>\n",
       "      <td>0.342521</td>\n",
       "      <td>0.171550</td>\n",
       "      <td>0.496976</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>0.264197</td>\n",
       "      <td>0.231399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.154259</td>\n",
       "      <td>0.315563</td>\n",
       "      <td>0.416829</td>\n",
       "      <td>0.360992</td>\n",
       "      <td>0.139828</td>\n",
       "      <td>0.297453</td>\n",
       "      <td>0.154008</td>\n",
       "      <td>0.226742</td>\n",
       "      <td>0.500149</td>\n",
       "      <td>0.111488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365414</td>\n",
       "      <td>0.326279</td>\n",
       "      <td>0.239293</td>\n",
       "      <td>0.069527</td>\n",
       "      <td>0.268340</td>\n",
       "      <td>-0.024531</td>\n",
       "      <td>0.371548</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.373378</td>\n",
       "      <td>0.260677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.464540</td>\n",
       "      <td>0.205301</td>\n",
       "      <td>0.169766</td>\n",
       "      <td>0.115828</td>\n",
       "      <td>0.339901</td>\n",
       "      <td>0.516269</td>\n",
       "      <td>0.154083</td>\n",
       "      <td>0.244009</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>0.240681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156384</td>\n",
       "      <td>0.347126</td>\n",
       "      <td>0.279128</td>\n",
       "      <td>0.192447</td>\n",
       "      <td>0.393713</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>0.421795</td>\n",
       "      <td>0.311139</td>\n",
       "      <td>0.338107</td>\n",
       "      <td>0.257135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.406527</td>\n",
       "      <td>0.589596</td>\n",
       "      <td>0.380854</td>\n",
       "      <td>0.028387</td>\n",
       "      <td>0.229991</td>\n",
       "      <td>0.320032</td>\n",
       "      <td>0.299893</td>\n",
       "      <td>0.269051</td>\n",
       "      <td>0.191915</td>\n",
       "      <td>0.387130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308263</td>\n",
       "      <td>0.378137</td>\n",
       "      <td>0.227571</td>\n",
       "      <td>0.046854</td>\n",
       "      <td>0.345195</td>\n",
       "      <td>0.169526</td>\n",
       "      <td>0.325486</td>\n",
       "      <td>0.364819</td>\n",
       "      <td>0.428112</td>\n",
       "      <td>0.255994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0      0.386197      0.430119        0.386089   0.245277         0.234082   \n",
       "1      0.369419      0.206298        0.352262   0.226191         0.240948   \n",
       "2      0.329358      0.441639        0.310998   0.288521         0.152576   \n",
       "3      0.565993      0.066067        0.350571   0.216517         0.105980   \n",
       "4      0.474208      0.283670        0.421432   0.250591         0.287740   \n",
       "5      0.468026      0.223539        0.122982   0.206050         0.380983   \n",
       "6      0.201679      0.381338        0.298234   0.213378         0.341426   \n",
       "7      0.315498      0.358282        0.249849   0.078857         0.210207   \n",
       "8      0.321690      0.219130        0.257058   0.207339         0.166509   \n",
       "9      0.242951      0.613820        0.551912   0.537407         0.394751   \n",
       "10     0.248624      0.320875        0.214544  -0.043706         0.211111   \n",
       "11     0.415100      0.344164        0.253655   0.264736         0.353636   \n",
       "12     0.260474      0.298087        0.417001   0.262082         0.267811   \n",
       "13     0.111177      0.300627        0.130324   0.009941         0.320916   \n",
       "14     0.216181      0.229471        0.249125   0.129681         0.130509   \n",
       "15     0.467237      0.262001        0.257967   0.351264         0.248651   \n",
       "16     0.204724      0.156506        0.167053   0.102885         0.115033   \n",
       "17     0.205129      0.283421        0.377584   0.193089         0.276089   \n",
       "18     0.145191      0.202475        0.296088   0.080199         0.304828   \n",
       "19     0.272093      0.360578        0.332876   0.150166         0.406546   \n",
       "20     0.237796      0.511094        0.256828   0.069686         0.155274   \n",
       "21     0.564727      0.533837        0.545933   0.376148         0.342575   \n",
       "22     0.336237      0.103055        0.152478   0.365210         0.439378   \n",
       "23     0.178119      0.253582        0.152957   0.183641         0.250289   \n",
       "24     0.439003      0.581463        0.413993   0.040530         0.256457   \n",
       "25     0.318287      0.224534        0.164119   0.129373         0.152149   \n",
       "26     0.599791      0.370321        0.645501   0.323689         0.684305   \n",
       "27     0.426911      0.327374        0.335961   0.206706         0.578716   \n",
       "28     0.172309      0.390668        0.273558   0.156306         0.448302   \n",
       "29     0.358740      0.132950        0.165707   0.145584         0.472086   \n",
       "30     0.625863      0.252566        0.396583   0.405398         0.157031   \n",
       "31     0.556992      0.446144        0.358348   0.275166         0.503915   \n",
       "32     0.442738      0.346256        0.396414   0.360849         0.274120   \n",
       "33     0.630434      0.417026        0.368123   0.283383         0.364684   \n",
       "34     0.545591      0.287892        0.271717   0.364813         0.394602   \n",
       "35     0.336157      0.290638        0.363282   0.180508         0.213293   \n",
       "36     0.370611      0.357193        0.273274   0.477726         0.081189   \n",
       "37     0.154259      0.315563        0.416829   0.360992         0.139828   \n",
       "38     0.464540      0.205301        0.169766   0.115828         0.339901   \n",
       "39     0.406527      0.589596        0.380854   0.028387         0.229991   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.319339        0.050656             0.200825       0.389005   \n",
       "1           0.174220        0.039027            -0.108835       0.318523   \n",
       "2           0.256034        0.351475             0.297466       0.244187   \n",
       "3           0.229419        0.096651            -0.050360       0.425980   \n",
       "4           0.293671        0.250811             0.390907       0.539080   \n",
       "5           0.358458        0.310780             0.284803       0.272772   \n",
       "6           0.403945        0.118311             0.188690       0.530882   \n",
       "7           0.282692        0.230639             0.143786       0.396897   \n",
       "8           0.149121        0.083959             0.054531       0.158812   \n",
       "9           0.415202        0.280240             0.329586       0.237438   \n",
       "10          0.266619        0.246094             0.279938       0.334327   \n",
       "11          0.419368        0.267184             0.151145       0.437623   \n",
       "12          0.364704        0.263458             0.276311       0.404660   \n",
       "13          0.178315       -0.025656             0.132050       0.248182   \n",
       "14          0.222792        0.066667             0.120956       0.254522   \n",
       "15          0.436804        0.299434             0.135787       0.713189   \n",
       "16          0.102750        0.121288            -0.056602       0.190979   \n",
       "17          0.294265        0.053723             0.232653       0.385599   \n",
       "18          0.251444        0.279713             0.114896       0.331973   \n",
       "19          0.223171        0.230316             0.111944       0.356213   \n",
       "20          0.320165        0.012778             0.118073       0.413859   \n",
       "21          0.348650        0.398068             0.456945       0.567430   \n",
       "22          0.332149        0.189341             0.396250       0.344556   \n",
       "23          0.175728        0.168876             0.157699       0.140369   \n",
       "24          0.431809        0.352558             0.138457       0.460704   \n",
       "25          0.173476        0.152377             0.147458       0.298457   \n",
       "26          0.601636        0.703182             0.191165       0.661400   \n",
       "27          0.468398        0.239256             0.385035       0.527718   \n",
       "28          0.273477        0.229244             0.201789       0.497279   \n",
       "29          0.362978        0.288241             0.264629       0.497745   \n",
       "30          0.383481        0.169056             0.274989       0.143623   \n",
       "31          0.515874        0.177182             0.419661       0.506074   \n",
       "32          0.326547        0.209490             0.245551       0.236719   \n",
       "33          0.385365        0.136257             0.462644       0.561446   \n",
       "34          0.500491        0.184913             0.406670       0.743301   \n",
       "35          0.266652        0.421781             0.283812       0.303687   \n",
       "36          0.290937        0.057073             0.261683       0.479012   \n",
       "37          0.297453        0.154008             0.226742       0.500149   \n",
       "38          0.516269        0.154083             0.244009       0.363665   \n",
       "39          0.320032        0.299893             0.269051       0.191915   \n",
       "\n",
       "    mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.287857  ...      0.314550       0.276671         0.229163   \n",
       "1                 0.151682  ...      0.052576       0.273266         0.243587   \n",
       "2                 0.452779  ...      0.205266       0.361498         0.389768   \n",
       "3                 0.322431  ...      0.067845       0.194928         0.107569   \n",
       "4                 0.227571  ...      0.253736       0.233148         0.364838   \n",
       "5                 0.461433  ...      0.147190       0.377790         0.269374   \n",
       "6                 0.198372  ...      0.033980       0.337153         0.319736   \n",
       "7                 0.335360  ...      0.384013       0.240521         0.179763   \n",
       "8                 0.320911  ...      0.197660       0.114888         0.280592   \n",
       "9                 0.722003  ...      0.298275       0.395009         0.335233   \n",
       "10                0.309187  ...      0.197058       0.342753         0.217666   \n",
       "11                0.516863  ...      0.163153       0.367720         0.041474   \n",
       "12                0.206999  ...      0.522413       0.277560         0.355798   \n",
       "13                0.270094  ...      0.197470       0.182067         0.019456   \n",
       "14                0.382204  ...      0.052249       0.369519         0.159547   \n",
       "15                0.332204  ...      0.070565       0.262594         0.233038   \n",
       "16                0.172960  ...      0.084964       0.292609         0.116040   \n",
       "17                0.200287  ...      0.128811       0.257674         0.190810   \n",
       "18                0.324658  ...      0.348772       0.105050         0.304204   \n",
       "19                0.324140  ...      0.280611       0.268189         0.359625   \n",
       "20                0.378171  ...      0.155993       0.500255         0.001719   \n",
       "21                0.311718  ...      0.571587       0.610831         0.534961   \n",
       "22                0.302989  ...      0.308640       0.370322         0.311813   \n",
       "23                0.014595  ...      0.254024       0.271049         0.279352   \n",
       "24                0.449049  ...      0.137508       0.558278        -0.032345   \n",
       "25                0.275234  ...      0.224574       0.182682         0.183465   \n",
       "26                0.526681  ...      0.188982       0.705023         0.257867   \n",
       "27                0.477764  ...      0.452361       0.286508         0.395223   \n",
       "28                0.369095  ...     -0.034141       0.222412         0.194965   \n",
       "29                0.349776  ...      0.125465       0.146489         0.368904   \n",
       "30                0.442094  ...      0.155846       0.452356         0.132736   \n",
       "31                0.344232  ...      0.357109       0.582543         0.410344   \n",
       "32                0.219817  ...      0.246329       0.421096         0.497765   \n",
       "33                0.049611  ...      0.499782       0.292308         0.314338   \n",
       "34                0.261611  ...      0.428071       0.250202         0.372881   \n",
       "35                0.151520  ...      0.340252       0.389241         0.490092   \n",
       "36                0.337940  ...      0.269190       0.197426         0.241530   \n",
       "37                0.111488  ...      0.365414       0.326279         0.239293   \n",
       "38                0.240681  ...      0.156384       0.347126         0.279128   \n",
       "39                0.387130  ...      0.308263       0.378137         0.227571   \n",
       "\n",
       "    worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0     0.188168          0.449392           0.319546         0.446428   \n",
       "1     0.089666          0.254069           0.121691         0.252544   \n",
       "2     0.294411          0.332537           0.185718         0.281220   \n",
       "3     0.262432          0.151166           0.201589         0.287575   \n",
       "4     0.314779          0.241509           0.051218         0.235337   \n",
       "5     0.342860          0.234561           0.065906         0.140395   \n",
       "6    -0.059347          0.176994           0.364039         0.270082   \n",
       "7     0.223506          0.305219           0.131533         0.235471   \n",
       "8     0.062215          0.394140           0.191486         0.207845   \n",
       "9     0.233243          0.506370           0.377747         0.574603   \n",
       "10    0.258519          0.299003           0.139554         0.101857   \n",
       "11    0.141381          0.412040           0.119012         0.468953   \n",
       "12    0.135159          0.114075           0.618796         0.502527   \n",
       "13    0.055345          0.285406           0.178283         0.315887   \n",
       "14    0.195806          0.472969           0.151230         0.246154   \n",
       "15    0.108998          0.325547           0.295953         0.376409   \n",
       "16   -0.052918          0.185662           0.005776         0.134715   \n",
       "17    0.361980          0.359155           0.166647         0.308206   \n",
       "18    0.048081          0.381806           0.082847         0.096901   \n",
       "19    0.111875          0.239295           0.134976         0.271283   \n",
       "20    0.168960          0.151737           0.046224         0.160137   \n",
       "21    0.440498          0.564862           0.183993         0.403239   \n",
       "22    0.333500          0.276649           0.306723         0.235396   \n",
       "23    0.010098          0.310728           0.239477         0.222729   \n",
       "24   -0.014140          0.624411           0.007767         0.488920   \n",
       "25    0.073260          0.259364           0.135196         0.184464   \n",
       "26    0.234057          0.665222           0.154083         0.331064   \n",
       "27    0.264869          0.533061           0.276267         0.642933   \n",
       "28    0.046520          0.392622           0.160810         0.153244   \n",
       "29   -0.110773          0.598762           0.043465         0.142770   \n",
       "30    0.218936          0.337666           0.159508         0.296501   \n",
       "31    0.396981          0.442774           0.332416         0.691754   \n",
       "32    0.233890          0.341040           0.231478         0.436393   \n",
       "33    0.242399          0.358231           0.039709         0.558835   \n",
       "34    0.144250          0.586456           0.320601         0.470379   \n",
       "35    0.014479          0.349001           0.132289         0.202058   \n",
       "36    0.190675          0.342521           0.171550         0.496976   \n",
       "37    0.069527          0.268340          -0.024531         0.371548   \n",
       "38    0.192447          0.393713           0.009027         0.421795   \n",
       "39    0.046854          0.345195           0.169526         0.325486   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  \n",
       "0               0.504266        0.293903                 0.366978  \n",
       "1               0.340169        0.279351                 0.122819  \n",
       "2               0.533877        0.296112                 0.085709  \n",
       "3               0.658456       -0.045699                 0.114189  \n",
       "4               0.520116        0.279817                 0.231730  \n",
       "5               0.375930        0.278314                 0.108154  \n",
       "6               0.308225        0.127975                 0.185774  \n",
       "7               0.257324        0.100528                 0.276636  \n",
       "8               0.041907        0.287700                 0.054814  \n",
       "9               0.743009        0.386556                 0.318300  \n",
       "10              0.483200        0.177158                 0.194537  \n",
       "11              0.196210        0.220770                 0.244765  \n",
       "12              0.569695        0.108649                 0.468425  \n",
       "13              0.149277        0.170247                 0.277363  \n",
       "14              0.293819        0.360581                 0.195601  \n",
       "15              0.475550        0.368765                 0.331211  \n",
       "16              0.198203        0.213967                 0.096693  \n",
       "17              0.483131        0.134870                 0.271625  \n",
       "18              0.410349        0.023723                 0.098133  \n",
       "19              0.468120        0.172372                 0.249658  \n",
       "20              0.186234        0.131438                -0.007395  \n",
       "21              0.554745        0.333165                 0.449125  \n",
       "22              0.496657        0.293125                 0.148750  \n",
       "23              0.408675        0.143942                 0.182837  \n",
       "24              0.384686        0.494755                 0.376439  \n",
       "25              0.222704        0.191220                 0.256525  \n",
       "26              0.568246        0.182178                 0.134998  \n",
       "27              0.481264        0.319794                 0.413560  \n",
       "28              0.467519        0.242481                 0.294280  \n",
       "29              0.503588        0.377347                 0.203947  \n",
       "30              0.605024        0.524569                 0.279083  \n",
       "31              0.561902        0.436272                 0.354933  \n",
       "32              0.329454        0.404347                 0.309656  \n",
       "33              0.567085        0.282957                 0.424783  \n",
       "34              0.405429        0.262271                 0.362671  \n",
       "35              0.208451        0.277488                 0.241132  \n",
       "36              0.204009        0.264197                 0.231399  \n",
       "37              0.465079        0.373378                 0.260677  \n",
       "38              0.311139        0.338107                 0.257135  \n",
       "39              0.364819        0.428112                 0.255994  \n",
       "\n",
       "[40 rows x 30 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data = generate_data(generator, 1, 40)\n",
    "generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 998us/step\n"
     ]
    }
   ],
   "source": [
    "# Generate 50 instances for each class\n",
    "synthetic_data_class_0 = generate_data(generator, 0, 50)\n",
    "synthetic_data_class_1 = generate_data(generator, 1, 50)\n",
    "\n",
    "# Combine all synthetic data into a single DataFrame and apply inverse transform to bring it back to original scale\n",
    "synthetic_data = pd.concat([synthetic_data_class_0, synthetic_data_class_1], ignore_index=True)\n",
    "synthetic_data = pd.DataFrame(scaler.inverse_transform(synthetic_data),columns=iris.feature_names)\n",
    "\n",
    "# Create corresponding class labels\n",
    "synthetic_labels = [0]*50 + [1]*50 \n",
    "\n",
    "# Add labels to the synthetic data\n",
    "synthetic_data['class'] = synthetic_labels\n",
    "\n",
    "# Save synthetic data as a CSV file\n",
    "synthetic_data.to_csv(r'C:\\Users\\lia68085\\test\\synthetic_caneer_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset from sklearn\n",
    "iris = datasets.load_breast_cancer()\n",
    "\n",
    "real_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "real_data['class'] = iris.target\n",
    "feature_names= iris.feature_names\n",
    "# Load the synthetic dataset\n",
    "synthetic_data = pd.read_csv(r'C:\\Users\\lia68085\\test\\synthetic_caneer_data.csv')\n",
    "\n",
    "# For each feature, create a histogram for the real and synthetic data\n",
    "for feature in feature_names:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(real_data[feature], bins=20, alpha=0.5, color='g', label='Real')\n",
    "    plt.title(f\"Real Data: {feature}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(synthetic_data[feature], bins=20, alpha=0.5, color='b', label='Synthetic')\n",
    "    plt.title(f\"Synthetic Data: {feature}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "# Print the summary statistics for the real and synthetic data\n",
    "print(\"Summary statistics for the real data:\")\n",
    "print(real_data.describe())\n",
    "print(\"\\nSummary statistics for the synthetic data:\")\n",
    "print(synthetic_data.describe())\n",
    "\n",
    "# For each pair of features, create a scatter plot for the real and synthetic data\n",
    "features = feature_names\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(real_data[features[i]], real_data[features[j]], alpha=0.5, color='g')\n",
    "        plt.title(f\"Real Data: {features[i]} vs {features[j]}\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(synthetic_data[features[i]], synthetic_data[features[j]], alpha=0.5, color='b')\n",
    "        plt.title(f\"Synthetic Data: {features[i]} vs {features[j]}\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[9 3]\n",
      " [1 7]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.70      0.88      0.78         8\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.80      0.81      0.80        20\n",
      "weighted avg       0.82      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the synthetic dataset\n",
    "synthetic_data = pd.read_csv(r'C:\\Users\\lia68085\\test\\synthetic_caneer_data.csv')\n",
    "\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = synthetic_data.drop(columns=['class'])\n",
    "y = synthetic_data['class']\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a K-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the real data:  0.7785588752196837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset from sklearn\n",
    "iris = datasets.load_breast_cancer()\n",
    "real_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "real_labels = iris.target\n",
    "\n",
    "# Standardize the real data\n",
    "real_data = scaler.transform(real_data)\n",
    "\n",
    "# Use the trained KNN model to make predictions on the real data\n",
    "real_pred = knn.predict(real_data)\n",
    "\n",
    "# Calculate the accuracy of the model on the real data\n",
    "accuracy = accuracy_score(real_labels, real_pred)\n",
    "print(\"Accuracy of the model on the real data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model trained on real data:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(real_data, real_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler_real = StandardScaler()\n",
    "X_train_real = scaler_real.fit_transform(X_train_real)\n",
    "X_test_real = scaler_real.transform(X_test_real)\n",
    "\n",
    "# Train a K-Nearest Neighbors model on the real data\n",
    "knn_real = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_real.fit(X_train_real, y_train_real)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred_real = knn_real.predict(X_test_real)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_real = accuracy_score(y_test_real, y_pred_real)\n",
    "print(\"Accuracy of the model trained on real data: \", accuracy_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model trained on synthetic data:  0.868421052631579\n",
      "Accuracy of the model trained on real data:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the real test data with the model trained on synthetic data\n",
    "y_pred_synthetic_model = knn.predict(X_test_real)\n",
    "\n",
    "# Evaluate the model trained on synthetic data\n",
    "accuracy_synthetic_model = accuracy_score(y_test_real, y_pred_synthetic_model)\n",
    "print(\"Accuracy of the model trained on synthetic data: \", accuracy_synthetic_model)\n",
    "\n",
    "# Make predictions on the real test data with the model trained on real data\n",
    "y_pred_real_model = knn_real.predict(X_test_real)\n",
    "\n",
    "# Evaluate the model trained on real data\n",
    "accuracy_real_model = accuracy_score(y_test_real, y_pred_real_model)\n",
    "print(\"Accuracy of the model trained on real data: \", accuracy_real_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
