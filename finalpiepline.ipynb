{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump,load\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['date', 'Schadensklasse', 'Effektivwert_z', 'Median_z', 'Mittlere_Absolute_Abweichung_z', 'Mittelwert_z', 'Median_y', 'Effektivwert_y', 'Effektivwert_x', 'Variance_z', 'Mittelwert_x', 'Standardabweichung_z']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Schadensklasse</th>\n",
       "      <th>Effektivwert_z</th>\n",
       "      <th>Median_z</th>\n",
       "      <th>Mittlere_Absolute_Abweichung_z</th>\n",
       "      <th>Mittelwert_z</th>\n",
       "      <th>Median_y</th>\n",
       "      <th>Effektivwert_y</th>\n",
       "      <th>Effektivwert_x</th>\n",
       "      <th>Variance_z</th>\n",
       "      <th>Mittelwert_x</th>\n",
       "      <th>Standardabweichung_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15896.6</td>\n",
       "      <td>15872</td>\n",
       "      <td>350.131</td>\n",
       "      <td>15889.7</td>\n",
       "      <td>-512</td>\n",
       "      <td>571.115</td>\n",
       "      <td>657.526</td>\n",
       "      <td>220962</td>\n",
       "      <td>569.600</td>\n",
       "      <td>469.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15887.1</td>\n",
       "      <td>15872</td>\n",
       "      <td>363.095</td>\n",
       "      <td>15879.7</td>\n",
       "      <td>-512</td>\n",
       "      <td>571.746</td>\n",
       "      <td>627.592</td>\n",
       "      <td>235057</td>\n",
       "      <td>545.792</td>\n",
       "      <td>484.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15893.9</td>\n",
       "      <td>15872</td>\n",
       "      <td>395.262</td>\n",
       "      <td>15885.8</td>\n",
       "      <td>-512</td>\n",
       "      <td>566.507</td>\n",
       "      <td>651.770</td>\n",
       "      <td>256573</td>\n",
       "      <td>565.248</td>\n",
       "      <td>506.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15927.7</td>\n",
       "      <td>15872</td>\n",
       "      <td>389.710</td>\n",
       "      <td>15920.1</td>\n",
       "      <td>-512</td>\n",
       "      <td>568.931</td>\n",
       "      <td>653.578</td>\n",
       "      <td>241982</td>\n",
       "      <td>566.784</td>\n",
       "      <td>491.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15925.5</td>\n",
       "      <td>15872</td>\n",
       "      <td>414.196</td>\n",
       "      <td>15917.1</td>\n",
       "      <td>-512</td>\n",
       "      <td>586.402</td>\n",
       "      <td>661.006</td>\n",
       "      <td>268509</td>\n",
       "      <td>567.040</td>\n",
       "      <td>517.919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  Schadensklasse  Effektivwert_z  Median_z  \\\n",
       "0 2020-11-04 11:40:00               2         15896.6     15872   \n",
       "1 2020-11-04 11:40:00               2         15887.1     15872   \n",
       "2 2020-11-04 11:40:00               2         15893.9     15872   \n",
       "3 2020-11-04 11:40:00               2         15927.7     15872   \n",
       "4 2020-11-04 11:40:00               2         15925.5     15872   \n",
       "\n",
       "   Mittlere_Absolute_Abweichung_z  Mittelwert_z  Median_y  Effektivwert_y  \\\n",
       "0                         350.131       15889.7      -512         571.115   \n",
       "1                         363.095       15879.7      -512         571.746   \n",
       "2                         395.262       15885.8      -512         566.507   \n",
       "3                         389.710       15920.1      -512         568.931   \n",
       "4                         414.196       15917.1      -512         586.402   \n",
       "\n",
       "   Effektivwert_x  Variance_z  Mittelwert_x  Standardabweichung_z  \n",
       "0         657.526      220962       569.600               469.831  \n",
       "1         627.592      235057       545.792               484.584  \n",
       "2         651.770      256573       565.248               506.277  \n",
       "3         653.578      241982       566.784               491.670  \n",
       "4         661.006      268509       567.040               517.919  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#load data and feature_important_axis1\n",
    "datapath=r'Dataanalyse/axis2_demo_tablepart1_0.csv'\n",
    "import json\n",
    "with open('syntheic_data/feature_important_axis1.json') as f:\n",
    "    feature_important_axis1 = json.load(f)\n",
    "\n",
    "feature_important_axis1\n",
    "df = pd.read_csv(datapath)\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset=[df.columns[0]], inplace=True)\n",
    "df[\"date\"] = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "features=[\"date\",'Schadensklasse']+feature_important_axis1\n",
    "\n",
    "print(features)\n",
    "data_selected = df[features]\n",
    "data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of batches: %d 2201\n",
      "length of batches: %d 2348\n",
      "length of batches: %d 2692\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# define the length\n",
    "alltime=np.unique(data_selected.date)\n",
    "alltime\n",
    "after=[]\n",
    "for x in alltime:\n",
    "    length=len(data_selected[data_selected[\"date\"]==x])\n",
    "    if length >=2000:\n",
    "        after.append(x)\n",
    "        print('length of batches: %d',length)\n",
    "print('='*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the last batch \n",
    "for time in after:\n",
    "    finaldata=data_selected[data_selected[\"date\"]==time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2692, 10)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=finaldata[feature_important_axis1].to_numpy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(test)\n",
    "print(len(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truedata=data_selected[data_selected[\"date\"]==time]\n",
    "true=truedata.Schadensklasse.to_list()\n",
    "true=np.array(true)\n",
    "print(len(true))\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n",
      "2255\n",
      "相同元素的比例: 0.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check the length of two arrays\n",
    "if len(res) != len(true):\n",
    "    raise ValueError(\"两个数组的长度不一致。\")\n",
    "\n",
    "# compare two arrays\n",
    "similarity_array = res == true\n",
    "print(similarity_array)\n",
    "# calculate the number of similar elements\n",
    "similar_count = np.sum(similarity_array)\n",
    "print(similar_count)\n",
    "# calculate the similarity ratio\n",
    "similarity_ratio = similar_count / len(res)\n",
    "\n",
    "print(f\"相同元素的比例: {similarity_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Configs:\n",
    "    output_attention = False\n",
    "    enc_in = 10\n",
    "    d_model = 512\n",
    "    embed = 'timeF'\n",
    "    freq = 's'\n",
    "    dropout = 0.05\n",
    "    e_layers = 2\n",
    "    c_out = 10\n",
    "    seq_len=200\n",
    "    label_len=100\n",
    "    pred_len=200\n",
    "    step_size=pred_len\n",
    "    model='Informer'\n",
    "    dec_in=10\n",
    "    factor=1\n",
    "    n_heads=8\n",
    "    d_ff=2048\n",
    "    activation='gelu'\n",
    "    distil=True\n",
    "    d_layers=1\n",
    "    batch_length=2000\n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the test data with length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 31)\n",
      "(110000, 12)\n",
      "length of batches: %d 8600\n",
      "length of batches: %d 13834\n",
      "length of batches: %d 17411\n",
      "length of batches: %d 17419\n",
      "length of batches: %d 17358\n",
      "length of batches: %d 17363\n",
      "length of batches: %d 16486\n",
      "==============================\n",
      "2021-05-25T10:53:20.000000000\n",
      "(200, 12)\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset numpy array length 2000\n",
    "import numpy as np  \n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import dump,load\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from Embed import DataEmbedding\n",
    "from datetime import timedelta\n",
    "from timefeatures import time_features\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "from models import Informer, Autoformer,LSTM,GRU\n",
    "\n",
    "\n",
    "\n",
    "datapath=r'Dataanalyse/all_data_axis1.csv'\n",
    "test_df = pd.read_csv(datapath)\n",
    "test_df.drop_duplicates(subset=[test_df.columns[0]], inplace=True)\n",
    "df = test_df.iloc[90000:200000].copy()\n",
    "print(df.shape)\n",
    "import json\n",
    "with open('syntheic_data/feature_important_axis1.json') as f:\n",
    "    feature_important_axis1 = json.load(f)\n",
    "df[\"date\"] = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "features=[\"date\",'Schadensklasse']+feature_important_axis1\n",
    "data_selected = df[features]\n",
    "print(data_selected.shape)\n",
    "data=data_selected\n",
    "alltime=np.unique(data.date)\n",
    "after=[]\n",
    "for x in alltime:\n",
    "    length=len(data[data[\"date\"]==x])\n",
    "    damage=np.unique(data[data[\"date\"]==x].Schadensklasse)\n",
    "    if length >=configs.batch_length:\n",
    "        after.append(x)\n",
    "        print('length of batches: %d',length)\n",
    "print('='*30)\n",
    "# get the data\n",
    "time=after[1]\n",
    "print(time)\n",
    "\n",
    "finaldata=data[data[\"date\"]==time]\n",
    "finaldata=finaldata[:configs.seq_len]\n",
    "print(finaldata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataloader for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 12)\n",
      "length of the testing dataset after times : %d 400\n",
      "torch.Size([400, 6])\n",
      "number of dataset in one batch: %d 1\n",
      "==============================\n",
      "number of batches: %d 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the scaler\n",
    "Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "with open(Scaler_file, 'rb') as f:\n",
    "    full_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "#  Add the empty dataframe with double length for the original dataframe\n",
    "zeros_df = pd.DataFrame(np.zeros((configs.pred_len, 12)), columns=finaldata.columns)\n",
    "doubledata = pd.concat([finaldata, zeros_df], ignore_index=True)\n",
    "print(doubledata.shape)  # shape of the output should be (400,10)\n",
    "start_date = doubledata['date'].iloc[0]\n",
    "# Generate a time series, starting from the start date, increasing every second, with the same length as data\n",
    "time_series = [start_date + timedelta(seconds=i) for i in range(len(doubledata))]\n",
    "# Assign the generated time series back to the 'date' column of data\n",
    "doubledata.loc[:, 'date'] = time_series\n",
    "\n",
    "dataset = doubledata\n",
    "print('length of the testing dataset after times : %d', len(dataset))\n",
    "df_stamp = dataset['date']\n",
    "df_stamp.date = pd.to_datetime(df_stamp)\n",
    "\n",
    "data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "data_stamp = torch.FloatTensor(data_stamp)\n",
    "print(data_stamp.shape)\n",
    "\n",
    "dataset = dataset.copy()\n",
    "\n",
    "#dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "dataset = full_scaler.transform(dataset)\n",
    "dataset = torch.FloatTensor(dataset)\n",
    "\n",
    "\n",
    "# Constructing samples\n",
    "samples = []\n",
    "Dte_samples=[]\n",
    "count = 0\n",
    "for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "    s_begin = index\n",
    "    s_end = s_begin + configs.seq_len # 200/1000\n",
    "    r_begin = s_end - configs.label_len # 100/500\n",
    "    r_end = r_begin + configs.label_len + configs.pred_len #300/1500\n",
    "    seq_x = dataset[s_begin:s_end] # 0-200/0-1000\n",
    "    seq_y = dataset[r_begin:r_end] # 100-300/500-1500\n",
    "    seq_x_mark = data_stamp[s_begin:s_end] # 0-200/0-1000\n",
    "    seq_y_mark = data_stamp[r_begin:r_end] # 100-300/500-1500\n",
    "    samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print('number of dataset in one batch: %d', count)\n",
    "samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "print('='*30)\n",
    "Dte_samples.append(samples)\n",
    "print('number of batches: %d', len(samples))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "Dte_datasets = [CustomDataset(dataloader) for dataloader in Dte_samples]\n",
    "Dte_combined_dataset = ConcatDataset(Dte_datasets)\n",
    "Dte_combined_dataloader = DataLoader(Dte_combined_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def acquire_device():\n",
    "    device = torch.device('cpu')\n",
    "    print('Use CPU')\n",
    "    return device\n",
    "\n",
    "device = acquire_device()\n",
    "\n",
    "def predict( batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def _run_model():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        return outputs\n",
    "    outputs = _run_model()\n",
    "\n",
    "    f_dim = 0\n",
    "    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "    batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "    return outputs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n",
      "(1, 200, 10)\n",
      "(200, 10)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    'Autoformer': Autoformer,\n",
    "    'Informer': Informer,\n",
    "    'LSTM':LSTM,\n",
    "    'GRU':GRU\n",
    "}\n",
    "model = model_dict[configs.model].Model(configs).to(device)\n",
    "\n",
    "model_path = r\"D:\\result\\second\\axis1_Informer_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\checkpoints\\checkpoint.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        #print('batch_x shape',batch_x.shape)\n",
    "\n",
    "        outputs, batch_y = predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "        \n",
    "        print(outputs.shape)\n",
    "        # Make sure the tensor is on the CPU\n",
    "        outputs_cpu = outputs.cpu()\n",
    "        # Convert to NumPy array\n",
    "        outputs_numpy = outputs_cpu.detach().numpy()\n",
    "        # Print the shape of the NumPy array to confirm the conversion\n",
    "        print(outputs_numpy.shape)\n",
    "        outputs_numpy=full_scaler.inverse_transform(outputs_numpy.reshape(-1,10))\n",
    "\n",
    "        res=outputs_numpy#[1,:].reshape(1,10)\n",
    "        print(res.shape)\n",
    "        # load the axis1 model with 10 best features\n",
    "        model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "        res=model.predict(res)\n",
    "        print(res)\n",
    "        print(len(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    output_attention = False\n",
    "    enc_in = 10\n",
    "    d_model = 512\n",
    "    embed = 'timeF'\n",
    "    freq = 's'\n",
    "    dropout = 0.05\n",
    "    e_layers = 2\n",
    "    c_out = 10\n",
    "    seq_len=200\n",
    "    label_len=100\n",
    "    pred_len=200\n",
    "    step_size=pred_len\n",
    "    model='Informer'\n",
    "    dec_in=10\n",
    "    factor=1\n",
    "    n_heads=8\n",
    "    d_ff=2048\n",
    "    activation='gelu'\n",
    "    distil=True\n",
    "    d_layers=1\n",
    "    batch_length=2000\n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1771047, 31)\n",
      "(1771047, 12)\n",
      "length of batches: %d 14054\n",
      "length of batches: %d 17530\n",
      "length of batches: %d 17405\n",
      "length of batches: %d 17397\n",
      "length of batches: %d 17406\n",
      "length of batches: %d 12516\n",
      "length of batches: %d 13834\n",
      "length of batches: %d 17411\n",
      "length of batches: %d 17419\n",
      "length of batches: %d 17358\n",
      "length of batches: %d 17363\n",
      "length of batches: %d 16533\n",
      "length of batches: %d 13397\n",
      "length of batches: %d 17330\n",
      "length of batches: %d 17351\n",
      "length of batches: %d 17319\n",
      "length of batches: %d 17342\n",
      "length of batches: %d 17419\n",
      "length of batches: %d 17413\n",
      "length of batches: %d 17395\n",
      "length of batches: %d 10288\n",
      "length of batches: %d 16685\n",
      "length of batches: %d 17357\n",
      "length of batches: %d 12804\n",
      "length of batches: %d 3217\n",
      "length of batches: %d 7583\n",
      "length of batches: %d 7579\n",
      "length of batches: %d 7582\n",
      "length of batches: %d 7692\n",
      "length of batches: %d 6647\n",
      "length of batches: %d 3828\n",
      "length of batches: %d 7591\n",
      "length of batches: %d 7584\n",
      "length of batches: %d 7588\n",
      "length of batches: %d 7587\n",
      "length of batches: %d 7580\n",
      "length of batches: %d 7695\n",
      "length of batches: %d 7586\n",
      "length of batches: %d 3924\n",
      "length of batches: %d 6951\n",
      "length of batches: %d 7589\n",
      "length of batches: %d 7591\n",
      "length of batches: %d 7592\n",
      "length of batches: %d 7591\n",
      "length of batches: %d 7693\n",
      "length of batches: %d 7576\n",
      "length of batches: %d 7580\n",
      "length of batches: %d 3170\n",
      "length of batches: %d 7582\n",
      "length of batches: %d 6484\n",
      "length of batches: %d 6274\n",
      "length of batches: %d 6276\n",
      "length of batches: %d 6355\n",
      "length of batches: %d 6286\n",
      "length of batches: %d 6283\n",
      "length of batches: %d 4964\n",
      "length of batches: %d 3268\n",
      "length of batches: %d 6274\n",
      "length of batches: %d 9257\n",
      "length of batches: %d 17124\n",
      "length of batches: %d 17345\n",
      "length of batches: %d 17403\n",
      "length of batches: %d 9304\n",
      "length of batches: %d 2908\n",
      "length of batches: %d 5326\n",
      "length of batches: %d 11606\n",
      "length of batches: %d 11727\n",
      "length of batches: %d 11589\n",
      "length of batches: %d 11595\n",
      "length of batches: %d 11591\n",
      "length of batches: %d 11564\n",
      "length of batches: %d 11350\n",
      "length of batches: %d 11565\n",
      "length of batches: %d 11727\n",
      "length of batches: %d 11572\n",
      "length of batches: %d 11604\n",
      "length of batches: %d 11583\n",
      "length of batches: %d 11582\n",
      "length of batches: %d 8664\n",
      "length of batches: %d 3283\n",
      "length of batches: %d 11487\n",
      "length of batches: %d 11712\n",
      "length of batches: %d 21073\n",
      "length of batches: %d 8332\n",
      "length of batches: %d 7276\n",
      "length of batches: %d 10560\n",
      "length of batches: %d 3649\n",
      "length of batches: %d 3573\n",
      "length of batches: %d 2582\n",
      "length of batches: %d 2728\n",
      "length of batches: %d 2835\n",
      "length of batches: %d 2767\n",
      "length of batches: %d 11519\n",
      "length of batches: %d 11648\n",
      "length of batches: %d 11654\n",
      "length of batches: %d 11651\n",
      "length of batches: %d 11670\n",
      "length of batches: %d 5005\n",
      "length of batches: %d 5599\n",
      "length of batches: %d 11652\n",
      "length of batches: %d 11804\n",
      "length of batches: %d 11640\n",
      "length of batches: %d 11660\n",
      "length of batches: %d 11657\n",
      "length of batches: %d 11671\n",
      "length of batches: %d 11654\n",
      "length of batches: %d 8938\n",
      "length of batches: %d 11802\n",
      "length of batches: %d 11656\n",
      "length of batches: %d 11642\n",
      "length of batches: %d 11664\n",
      "length of batches: %d 11648\n",
      "length of batches: %d 11645\n",
      "length of batches: %d 8271\n",
      "length of batches: %d 11822\n",
      "length of batches: %d 11647\n",
      "length of batches: %d 11538\n",
      "length of batches: %d 9779\n",
      "length of batches: %d 11535\n",
      "length of batches: %d 11562\n",
      "length of batches: %d 10890\n",
      "length of batches: %d 10875\n",
      "length of batches: %d 6902\n",
      "length of batches: %d 10852\n",
      "length of batches: %d 10839\n",
      "length of batches: %d 10875\n",
      "length of batches: %d 10871\n",
      "length of batches: %d 10878\n",
      "length of batches: %d 10896\n",
      "length of batches: %d 11010\n",
      "length of batches: %d 2845\n",
      "length of batches: %d 3124\n",
      "length of batches: %d 11175\n",
      "length of batches: %d 11651\n",
      "length of batches: %d 6499\n",
      "length of batches: %d 4980\n",
      "length of batches: %d 11017\n",
      "length of batches: %d 22385\n",
      "length of batches: %d 22761\n",
      "length of batches: %d 21707\n",
      "length of batches: %d 12211\n",
      "length of batches: %d 18601\n",
      "length of batches: %d 7414\n",
      "length of batches: %d 5156\n",
      "length of batches: %d 11811\n",
      "length of batches: %d 11684\n",
      "length of batches: %d 11669\n",
      "length of batches: %d 11679\n",
      "length of batches: %d 11652\n",
      "length of batches: %d 3302\n",
      "length of batches: %d 3301\n",
      "length of batches: %d 6432\n",
      "length of batches: %d 11617\n",
      "length of batches: %d 2875\n",
      "length of batches: %d 4675\n",
      "length of batches: %d 8989\n",
      "length of batches: %d 11507\n",
      "length of batches: %d 3291\n",
      "length of batches: %d 4783\n",
      "length of batches: %d 4971\n",
      "length of batches: %d 3458\n",
      "length of batches: %d 5082\n",
      "length of batches: %d 6545\n",
      "length of batches: %d 6741\n",
      "length of batches: %d 6656\n",
      "length of batches: %d 6664\n",
      "length of batches: %d 6657\n",
      "length of batches: %d 2946\n",
      "length of batches: %d 12601\n",
      "length of batches: %d 8078\n",
      "length of batches: %d 2132\n",
      "length of batches: %d 4935\n",
      "length of batches: %d 4943\n",
      "length of batches: %d 4926\n",
      "length of batches: %d 3557\n",
      "length of batches: %d 2591\n",
      "length of batches: %d 4974\n",
      "length of batches: %d 3925\n",
      "length of batches: %d 8747\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset numpy array length 2000\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from joblib import dump,load\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from Embed import DataEmbedding\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "# load the test dataset numpy array length 2000\n",
    "from timefeatures import time_features\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "from models import Informer, Autoformer,LSTM,GRU\n",
    "from datetime import timedelta\n",
    "from timefeatures import time_features\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "datapath=r'Dataanalyse/all_data_axis1.csv'\n",
    "test_df = pd.read_csv(datapath)\n",
    "test_df.drop_duplicates(subset=[test_df.columns[0]], inplace=True)\n",
    "df = test_df.copy()\n",
    "print(df.shape)\n",
    "import json\n",
    "with open('syntheic_data/feature_important_axis1.json') as f:\n",
    "    feature_important_axis1 = json.load(f)\n",
    "df[\"date\"] = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "features=[\"date\",'Schadensklasse']+feature_important_axis1\n",
    "data_selected = df[features]\n",
    "print(data_selected.shape)\n",
    "data=data_selected\n",
    "alltime=np.unique(data.date)\n",
    "after=[]\n",
    "for x in alltime:\n",
    "    length=len(data[data[\"date\"]==x])\n",
    "    damage=np.unique(data[data[\"date\"]==x].Schadensklasse)\n",
    "    if length >=configs.batch_length:\n",
    "        after.append(x)\n",
    "        print('length of batches: %d',length)\n",
    "print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the scaler\n",
    "Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "with open(Scaler_file, 'rb') as f:\n",
    "    full_scaler = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: %d 179\n"
     ]
    }
   ],
   "source": [
    "Dte_samples=[]\n",
    "for time in after:\n",
    "    finaldata=data[data[\"date\"]==time]\n",
    "    start_date = finaldata['date'].iloc[0]\n",
    "    # Generate a time series, starting from the start date, increasing every second, with the same length as data\n",
    "    time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "    # Assign the generated time series back to the 'date' column of data\n",
    "    finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "    dataset = finaldata\n",
    "    #print('length of the testing dataset after times 0.2: %d', len(dataset))\n",
    "\n",
    "    df_stamp = dataset['date']\n",
    "    df_stamp.date = pd.to_datetime(df_stamp)\n",
    "    data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "    data_stamp = torch.FloatTensor(data_stamp)\n",
    "    #print(data_stamp.shape)\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "    # schadenklasse = dataset['Schadensklasse']\n",
    "    dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date']))\n",
    "\n",
    "    if 'Schadensklasse' in dataset.columns:\n",
    "        withschaden = dataset.drop('Schadensklasse', axis=1)\n",
    "    else:\n",
    "        withschaden = dataset.copy()\n",
    "\n",
    "    trans_dataset = full_scaler.transform(withschaden)\n",
    "    trans_dataset = torch.FloatTensor(trans_dataset)\n",
    "\n",
    "    samples = []\n",
    "    count = 0\n",
    "    #print('range',range(0, len(dataset) - configs.seq_len - configs.pred_len + 1))\n",
    "    #print('step size',configs.step_size)\n",
    "    for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "        # train_x, x_mark, train_y, y_mark\n",
    "        s_begin = index\n",
    "        s_end = s_begin + configs.seq_len\n",
    "        r_begin = s_end - configs.label_len\n",
    "        r_end = r_begin + configs.label_len + configs.pred_len\n",
    "        damage=dataset['Schadensklasse'][r_begin:r_end].to_numpy()\n",
    "        damage = torch.LongTensor(damage)\n",
    "\n",
    "        #print(\"damage\",damage[10])\n",
    "        old_seq_x = dataset[s_begin:s_end].copy()\n",
    "        old_seq_y = dataset[r_begin:r_end].copy()\n",
    "\n",
    "        if 'Schadensklasse' in old_seq_x.columns:\n",
    "            seq_x = old_seq_x.drop('Schadensklasse', axis=1)\n",
    "        else:\n",
    "            seq_x = old_seq_x.copy()\n",
    "            \n",
    "        if 'Schadensklasse' in old_seq_y.columns:\n",
    "            seq_y = old_seq_y.drop('Schadensklasse', axis=1)\n",
    "        else:\n",
    "            seq_y = old_seq_y.copy()      \n",
    "\n",
    "\n",
    "        seq_x = full_scaler.transform(seq_x)\n",
    "        seq_y = full_scaler.transform(seq_y)\n",
    "\n",
    "        seq_x = torch.FloatTensor(seq_x)\n",
    "        seq_y = torch.FloatTensor(seq_y)\n",
    "\n",
    "        seq_x_mark = data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = data_stamp[r_begin:r_end]\n",
    "\n",
    "        samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark,damage))\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    #print('number of dataset in one batch: %d', count)\n",
    "    samplesloader = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "    #print('length of samples',len(samplesloader))\n",
    "    for x,y, z, p ,damge in samplesloader:\n",
    "       pass  \n",
    "    #print('the last shape',damge.shape)\n",
    "    \n",
    "    #print('='*30)\n",
    "    Dte_samples.append(samplesloader)\n",
    "print('number of batches: %d', len(Dte_samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8747, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8454\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "Dte_datasets = [CustomDataset(dataloader) for dataloader in Dte_samples]\n",
    "Dte_combined_dataset = ConcatDataset(Dte_datasets)\n",
    "Dte_combined_dataloader = DataLoader(Dte_combined_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(len(Dte_combined_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 300])\n",
      "532\n"
     ]
    }
   ],
   "source": [
    "for i, (x,y,z,p,da ) in enumerate(Dte_combined_dataloader) :\n",
    "    print(da.shape)\n",
    "    pass\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def acquire_device():\n",
    "    device = torch.device('cuda')\n",
    "    print('Use gpu')\n",
    "    return device\n",
    "\n",
    "device = acquire_device()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "from models import Informer, Autoformer,LSTM,GRU\n",
    "from datetime import timedelta\n",
    "from timefeatures import time_features\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "model_dict = {\n",
    "    'Autoformer': Autoformer,\n",
    "    'Informer': Informer,\n",
    "    'LSTM':LSTM,\n",
    "    'GRU':GRU\n",
    "}\n",
    "timeserie_model = model_dict[configs.model].Model(configs).to(device)\n",
    "\n",
    "model_path = r\"D:\\result\\second\\axis1_Informer_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\checkpoints\\checkpoint.pth\"\n",
    "\n",
    "timeserie_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "timeserie_model.eval()\n",
    "\n",
    "\n",
    "def acquire_device():\n",
    "    device = torch.device('cpu')\n",
    "    print('Use gpu')\n",
    "    return device\n",
    "\n",
    "device = acquire_device()\n",
    "\n",
    "def predict( batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def run_model():\n",
    "        outputs = timeserie_model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        return outputs\n",
    "    outputs = run_model()\n",
    "\n",
    "    f_dim = 0\n",
    "    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "    batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "    return outputs, batch_y\n",
    "\n",
    "\n",
    "pre=[]\n",
    "groundtrue=[]\n",
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark,truedamage) in enumerate(Dte_combined_dataloader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()#.to(device)\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        # print('batch_x shape',batch_x.shape)\n",
    "        #truedamage=truedamage.float().to(device)\n",
    "        outputs, batch_y = predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "        \n",
    "        # Convert to NumPy array\n",
    "        outputs_numpy = outputs.detach().numpy()\n",
    "        # Print the shape of the NumPy array to confirm the conversion\n",
    "        #print(outputs_numpy.shape)\n",
    "        outputs_numpy=full_scaler.inverse_transform(outputs_numpy.reshape(-1,10))\n",
    "\n",
    "        res=outputs_numpy#[1,:].reshape(1,10)\n",
    "\n",
    "\n",
    "        # load the axis1 model with 10 best features\n",
    "        model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "        predresult=model.predict(res)\n",
    "        #print(\"res\",predresult.shape)\n",
    "        pre.append(predresult)\n",
    "\n",
    "        true=truedamage[:,-configs.pred_len:]\n",
    "        output_true=true.detach().numpy().reshape(-1)\n",
    "        #print(\"true\",output_true.shape)\n",
    "        groundtrue.append(output_true)\n",
    "        #print(\"*\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8454\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "8454\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(pre))\n",
    "print(pre[0])\n",
    "print(len(groundtrue))\n",
    "print(groundtrue[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690800\n",
      "1690800\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# 使用列表推导式将二维列表转换为一维列表\n",
    "pre_flattened = [item for sublist in pre for item in sublist]\n",
    "groundtrue_flattened = [item for sublist in groundtrue for item in sublist]\n",
    "\n",
    "# 打印结果以确认\n",
    "print(len(pre_flattened))  # 应该输出 8400 (42 * 200)\n",
    "print(len(groundtrue_flattened))  # 应该输出 8400 (42 * 200)\n",
    "\n",
    "# 确认顺序不变\n",
    "print(pre_flattened[:200])  # 打印前10个元素确认顺序\n",
    "print(groundtrue_flattened[:200])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pre_flattened))\n",
    "print(type(groundtrue_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相同元素的比例: 0.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "pre_array = np.array(pre_flattened)\n",
    "groundtrue_array = np.array(groundtrue_flattened)\n",
    "\n",
    "# 检查两个数组的长度是否一致\n",
    "if len(pre_array) != len(groundtrue_array):\n",
    "    raise ValueError(\"两个数组的长度不一致。\")\n",
    "\n",
    "# 比较两个数组\n",
    "similarity_array = pre_array == groundtrue_array\n",
    "\n",
    "# 计算相同元素的数量\n",
    "similar_count = np.sum(similarity_array)\n",
    "\n",
    "# 计算相似度比例\n",
    "similarity_ratio = similar_count / len(pre_array)\n",
    "\n",
    "print(f\"相同元素的比例: {similarity_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "flattened_pre = [item for sublist in pre for item in sublist]\n",
    "\n",
    "print(flattened_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(flattened_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the testing dataset after times 0.2: %d 8600\n",
      "torch.Size([8600, 6])\n",
      "number of dataset in one batch: %d 42\n",
      "==============================\n",
      "number of batches: %d 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the scaler\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "with open(Scaler_file, 'rb') as f:\n",
    "    full_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "Dte_samples=[]\n",
    "finaldata=data[data[\"date\"]==time]\n",
    "start_date = finaldata['date'].iloc[0]\n",
    "# 生成一个时间序列，从起始日期开始，每秒递增，长度与data相同\n",
    "time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "# 将生成的时间序列赋值回data的'date'列\n",
    "finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "dataset = finaldata\n",
    "print('length of the testing dataset after times 0.2: %d', len(dataset))\n",
    "\n",
    "df_stamp = dataset['date']\n",
    "df_stamp.date = pd.to_datetime(df_stamp)\n",
    "data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "data_stamp = torch.FloatTensor(data_stamp)\n",
    "print(data_stamp.shape)\n",
    "\n",
    "dataset = dataset.copy()\n",
    "\n",
    "#dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "\n",
    "dataset = full_scaler.transform(dataset)\n",
    "dataset = torch.FloatTensor(dataset)\n",
    "# 构造样本\n",
    "samples = []\n",
    "count = 0\n",
    "for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "    # train_x, x_mark, train_y, y_mark\n",
    "    s_begin = index\n",
    "    s_end = s_begin + configs.seq_len\n",
    "    r_begin = s_end - configs.label_len\n",
    "    r_end = r_begin + configs.label_len + configs.pred_len\n",
    "    seq_x = dataset[s_begin:s_end]\n",
    "    seq_y = dataset[r_begin:r_end]\n",
    "    seq_x_mark = data_stamp[s_begin:s_end]\n",
    "    seq_y_mark = data_stamp[r_begin:r_end]\n",
    "    samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print('number of dataset in one batch: %d', count)\n",
    "samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "print('='*30)\n",
    "Dte_samples.append(samples)\n",
    "print('number of batches: %d', len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 10])\n",
      "torch.Size([32, 300, 10])\n",
      "torch.Size([32, 200, 6])\n",
      "torch.Size([32, 300, 6])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,p in samples:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(z.shape)\n",
    "    print(p.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta\n",
    "# from timefeatures import time_features\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "# # load the scaler\n",
    "# import pathlib as Path\n",
    "# import pickle\n",
    "# Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "# with open(Scaler_file, 'rb') as f:\n",
    "#     full_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "# Dte_samples=[]\n",
    "# for time in after:\n",
    "#     finaldata=data[data[\"date\"]==time]\n",
    "#     start_date = finaldata['date'].iloc[0]\n",
    "#     # 生成一个时间序列，从起始日期开始，每秒递增，长度与data相同\n",
    "#     time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "#     # 将生成的时间序列赋值回data的'date'列\n",
    "#     finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "#     dataset = finaldata\n",
    "#     print('length of the testing dataset after times 0.2: %d', len(dataset))\n",
    "\n",
    "#     df_stamp = dataset['date']\n",
    "#     df_stamp.date = pd.to_datetime(df_stamp)\n",
    "#     data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "#     data_stamp = torch.FloatTensor(data_stamp)\n",
    "#     print(data_stamp.shape)\n",
    "\n",
    "#     dataset = dataset.copy()\n",
    "\n",
    "#     dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "#     dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "\n",
    "#     dataset = full_scaler.transform(dataset)\n",
    "#     dataset = torch.FloatTensor(dataset)\n",
    "#     # 构造样本\n",
    "#     samples = []\n",
    "#     count = 0\n",
    "#     for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "#         # train_x, x_mark, train_y, y_mark\n",
    "#         s_begin = index\n",
    "#         s_end = s_begin + configs.seq_len\n",
    "#         r_begin = s_end - configs.label_len\n",
    "#         r_end = r_begin + configs.label_len + configs.pred_len\n",
    "#         seq_x = dataset[s_begin:s_end]\n",
    "#         seq_y = dataset[r_begin:r_end]\n",
    "#         seq_x_mark = data_stamp[s_begin:s_end]\n",
    "#         seq_y_mark = data_stamp[r_begin:r_end]\n",
    "#         samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "#         count += 1\n",
    "\n",
    "\n",
    "#     print('number of dataset in one batch: %d', count)\n",
    "#     samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "#     print('='*30)\n",
    "#     Dte_samples.append(samples)\n",
    "# print('number of batches: %d', len(Dte_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n",
      "0\n",
      "torch.Size([1, 200, 10])\n",
      "1\n",
      "torch.Size([1, 200, 10])\n",
      "2\n",
      "torch.Size([1, 200, 10])\n",
      "3\n",
      "torch.Size([1, 200, 10])\n",
      "4\n",
      "torch.Size([1, 200, 10])\n",
      "5\n",
      "torch.Size([1, 200, 10])\n",
      "6\n",
      "torch.Size([1, 200, 10])\n",
      "7\n",
      "torch.Size([1, 200, 10])\n",
      "8\n",
      "torch.Size([1, 200, 10])\n",
      "9\n",
      "torch.Size([1, 200, 10])\n",
      "10\n",
      "torch.Size([1, 200, 10])\n",
      "11\n",
      "torch.Size([1, 200, 10])\n",
      "12\n",
      "torch.Size([1, 200, 10])\n",
      "13\n",
      "torch.Size([1, 200, 10])\n",
      "14\n",
      "torch.Size([1, 200, 10])\n",
      "15\n",
      "torch.Size([1, 200, 10])\n",
      "16\n",
      "torch.Size([1, 200, 10])\n",
      "17\n",
      "torch.Size([1, 200, 10])\n",
      "18\n",
      "torch.Size([1, 200, 10])\n",
      "19\n",
      "torch.Size([1, 200, 10])\n",
      "20\n",
      "torch.Size([1, 200, 10])\n",
      "21\n",
      "torch.Size([1, 200, 10])\n",
      "22\n",
      "torch.Size([1, 200, 10])\n",
      "23\n",
      "torch.Size([1, 200, 10])\n",
      "24\n",
      "torch.Size([1, 200, 10])\n",
      "25\n",
      "torch.Size([1, 200, 10])\n",
      "26\n",
      "torch.Size([1, 200, 10])\n",
      "27\n",
      "torch.Size([1, 200, 10])\n",
      "28\n",
      "torch.Size([1, 200, 10])\n",
      "29\n",
      "torch.Size([1, 200, 10])\n",
      "30\n",
      "torch.Size([1, 200, 10])\n",
      "31\n",
      "torch.Size([1, 200, 10])\n",
      "32\n",
      "torch.Size([1, 200, 10])\n",
      "33\n",
      "torch.Size([1, 200, 10])\n",
      "34\n",
      "torch.Size([1, 200, 10])\n",
      "35\n",
      "torch.Size([1, 200, 10])\n",
      "36\n",
      "torch.Size([1, 200, 10])\n",
      "37\n",
      "torch.Size([1, 200, 10])\n",
      "38\n",
      "torch.Size([1, 200, 10])\n",
      "39\n",
      "torch.Size([1, 200, 10])\n",
      "40\n",
      "torch.Size([1, 200, 10])\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "    print(batch_x.shape)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from Embed import DataEmbedding\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "# Define your model architecture\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding layer\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=configs.d_model,\n",
    "            hidden_size=configs.d_model,\n",
    "            num_layers=configs.e_layers,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        num_directions = 1\n",
    "        self.projection = nn.Linear(configs.d_model * num_directions, configs.c_out)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask=None):\n",
    "        x_enc = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        lstm_out, _ = self.lstm(x_enc)\n",
    "        lstm_out = self.projection(lstm_out)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return lstm_out[:, -self.pred_len:, :], None\n",
    "        else:\n",
    "            return lstm_out[:, -self.pred_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "def acquire_device():\n",
    "    device = torch.device('cpu')\n",
    "    print('Use CPU')\n",
    "    return device\n",
    "device = acquire_device()\n",
    "import torch\n",
    "\n",
    "model_path = r\"D:\\result\\second\\axis2_LSTM_Custom_axis2_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\checkpoints\\checkpoint.pth\"\n",
    "model = Model(configs).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict( batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -200:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :100, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def _run_model():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        return outputs\n",
    "    outputs = _run_model()\n",
    "\n",
    "    f_dim = 0\n",
    "    outputs = outputs[:, -200:, f_dim:]\n",
    "    batch_y = batch_y[:, -200:, f_dim:].to(device)\n",
    "\n",
    "    return outputs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n"
     ]
    }
   ],
   "source": [
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        #print('batch_x shape',batch_x.shape)\n",
    "\n",
    "        outputs, batch_y = predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 10])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "# 确保张量在 CPU 上\n",
    "outputs_cpu = outputs[0].cpu()\n",
    "# 转换为 NumPy 数组\n",
    "outputs_numpy = outputs_cpu.detach().numpy()\n",
    "# 打印 NumPy 数组的形状以确认转换\n",
    "print(outputs_numpy.shape)\n",
    "outputs_numpy=full_scaler.inverse_transform(outputs_numpy)\n",
    "\n",
    "res=outputs_numpy[1,:].reshape(1,10)\n",
    "res.shape\n",
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=outputs_numpy[1,:].reshape(1,10)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
