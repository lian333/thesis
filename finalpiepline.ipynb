{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump,load\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['date', 'Schadensklasse', 'Effektivwert_z', 'Median_z', 'Mittlere_Absolute_Abweichung_z', 'Mittelwert_z', 'Median_y', 'Effektivwert_y', 'Effektivwert_x', 'Variance_z', 'Mittelwert_x', 'Standardabweichung_z']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Schadensklasse</th>\n",
       "      <th>Effektivwert_z</th>\n",
       "      <th>Median_z</th>\n",
       "      <th>Mittlere_Absolute_Abweichung_z</th>\n",
       "      <th>Mittelwert_z</th>\n",
       "      <th>Median_y</th>\n",
       "      <th>Effektivwert_y</th>\n",
       "      <th>Effektivwert_x</th>\n",
       "      <th>Variance_z</th>\n",
       "      <th>Mittelwert_x</th>\n",
       "      <th>Standardabweichung_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15896.6</td>\n",
       "      <td>15872</td>\n",
       "      <td>350.131</td>\n",
       "      <td>15889.7</td>\n",
       "      <td>-512</td>\n",
       "      <td>571.115</td>\n",
       "      <td>657.526</td>\n",
       "      <td>220962</td>\n",
       "      <td>569.600</td>\n",
       "      <td>469.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15887.1</td>\n",
       "      <td>15872</td>\n",
       "      <td>363.095</td>\n",
       "      <td>15879.7</td>\n",
       "      <td>-512</td>\n",
       "      <td>571.746</td>\n",
       "      <td>627.592</td>\n",
       "      <td>235057</td>\n",
       "      <td>545.792</td>\n",
       "      <td>484.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15893.9</td>\n",
       "      <td>15872</td>\n",
       "      <td>395.262</td>\n",
       "      <td>15885.8</td>\n",
       "      <td>-512</td>\n",
       "      <td>566.507</td>\n",
       "      <td>651.770</td>\n",
       "      <td>256573</td>\n",
       "      <td>565.248</td>\n",
       "      <td>506.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15927.7</td>\n",
       "      <td>15872</td>\n",
       "      <td>389.710</td>\n",
       "      <td>15920.1</td>\n",
       "      <td>-512</td>\n",
       "      <td>568.931</td>\n",
       "      <td>653.578</td>\n",
       "      <td>241982</td>\n",
       "      <td>566.784</td>\n",
       "      <td>491.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-04 11:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>15925.5</td>\n",
       "      <td>15872</td>\n",
       "      <td>414.196</td>\n",
       "      <td>15917.1</td>\n",
       "      <td>-512</td>\n",
       "      <td>586.402</td>\n",
       "      <td>661.006</td>\n",
       "      <td>268509</td>\n",
       "      <td>567.040</td>\n",
       "      <td>517.919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  Schadensklasse  Effektivwert_z  Median_z  \\\n",
       "0 2020-11-04 11:40:00               2         15896.6     15872   \n",
       "1 2020-11-04 11:40:00               2         15887.1     15872   \n",
       "2 2020-11-04 11:40:00               2         15893.9     15872   \n",
       "3 2020-11-04 11:40:00               2         15927.7     15872   \n",
       "4 2020-11-04 11:40:00               2         15925.5     15872   \n",
       "\n",
       "   Mittlere_Absolute_Abweichung_z  Mittelwert_z  Median_y  Effektivwert_y  \\\n",
       "0                         350.131       15889.7      -512         571.115   \n",
       "1                         363.095       15879.7      -512         571.746   \n",
       "2                         395.262       15885.8      -512         566.507   \n",
       "3                         389.710       15920.1      -512         568.931   \n",
       "4                         414.196       15917.1      -512         586.402   \n",
       "\n",
       "   Effektivwert_x  Variance_z  Mittelwert_x  Standardabweichung_z  \n",
       "0         657.526      220962       569.600               469.831  \n",
       "1         627.592      235057       545.792               484.584  \n",
       "2         651.770      256573       565.248               506.277  \n",
       "3         653.578      241982       566.784               491.670  \n",
       "4         661.006      268509       567.040               517.919  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#load data and feature_important_axis1\n",
    "datapath=r'../Dataanalyse/axis2_demo_tablepart1_0.csv'\n",
    "import json\n",
    "with open('feature_important_axis1.json') as f:\n",
    "    feature_important_axis1 = json.load(f)\n",
    "\n",
    "feature_important_axis1\n",
    "df = pd.read_csv(datapath)\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset=[df.columns[0]], inplace=True)\n",
    "df[\"date\"] = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "features=[\"date\",'Schadensklasse']+feature_important_axis1\n",
    "\n",
    "print(features)\n",
    "data_selected = df[features]\n",
    "data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of batches: %d 2201\n",
      "length of batches: %d 2348\n",
      "length of batches: %d 2692\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# define the length\n",
    "alltime=np.unique(data_selected.date)\n",
    "alltime\n",
    "after=[]\n",
    "for x in alltime:\n",
    "    length=len(data_selected[data_selected[\"date\"]==x])\n",
    "    if length >=2000:\n",
    "        after.append(x)\n",
    "        print('length of batches: %d',length)\n",
    "print('='*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the last batch \n",
    "for time in after:\n",
    "    finaldata=data_selected[data_selected[\"date\"]==time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2692, 10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=finaldata[feature_important_axis1].to_numpy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../10_best_xgboost_modeltestaxis1.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the axis1 model with 10 best features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../10_best_xgboost_modeltestaxis1.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m res\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res))\n",
      "File \u001b[1;32md:\\app\\conda_test\\finaltest\\lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../10_best_xgboost_modeltestaxis1.joblib'"
     ]
    }
   ],
   "source": [
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(test)\n",
    "print(len(res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "truedata=data_selected[data_selected[\"date\"]==time]\n",
    "true=truedata.Schadensklasse.to_list()\n",
    "true=np.array(true)\n",
    "print(len(true))\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check the length of two arrays\n",
    "if len(res) != len(true):\n",
    "    raise ValueError(\"两个数组的长度不一致。\")\n",
    "\n",
    "# compare two arrays\n",
    "similarity_array = res == true\n",
    "print(similarity_array)\n",
    "# calculate the number of similar elements\n",
    "similar_count = np.sum(similarity_array)\n",
    "print(similar_count)\n",
    "# calculate the similarity ratio\n",
    "similarity_ratio = similar_count / len(res)\n",
    "\n",
    "print(f\"相同元素的比例: {similarity_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    pred_len = 200\n",
    "    output_attention = False\n",
    "    enc_in = 10\n",
    "    d_model = 512\n",
    "    embed = 'timeF'\n",
    "    freq = 's'\n",
    "    dropout = 0.05\n",
    "    e_layers = 2\n",
    "    c_out = 10\n",
    "    seq_len=200\n",
    "    label_len=100\n",
    "    pred_len=200\n",
    "    step_size=200\n",
    "\n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 31)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test dataset numpy array length 2000\n",
    "import numpy as np  \n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import dump,load\n",
    "\n",
    "datapath=r'Dataanalyse/all_data_axis1.csv'\n",
    "test_df = pd.read_csv(datapath)\n",
    "test_df.drop_duplicates(subset=[test_df.columns[0]], inplace=True)\n",
    "df = test_df.iloc[90000:200000].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 12)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('syntheic_data/feature_important_axis1.json') as f:\n",
    "    feature_important_axis1 = json.load(f)\n",
    "df[\"date\"] = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "features=[\"date\",'Schadensklasse']+feature_important_axis1\n",
    "data_selected = df[features]\n",
    "data_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of batches: %d 8600\n",
      "length of batches: %d 13834\n",
      "length of batches: %d 17411\n",
      "length of batches: %d 17419\n",
      "length of batches: %d 17358\n",
      "length of batches: %d 17363\n",
      "length of batches: %d 16486\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "data=data_selected\n",
    "alltime=np.unique(data.date)\n",
    "after=[]\n",
    "for x in alltime:\n",
    "    length=len(data[data[\"date\"]==x])\n",
    "    damage=np.unique(data[data[\"date\"]==x].Schadensklasse)\n",
    "    # if length >10000 and len(damage)>=2:\n",
    "\n",
    "    if length >=2000:\n",
    "        after.append(x)\n",
    "        print('length of batches: %d',length)\n",
    "print('='*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2021-01-15T03:06:40.000000000')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time=after[0]\n",
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try single 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 12)\n",
      "length of the testing dataset after times : %d 400\n",
      "torch.Size([400, 6])\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from timefeatures import time_features\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "# load the scaler\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "\n",
    "Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "with open(Scaler_file, 'rb') as f:\n",
    "    full_scaler = pickle.load(f)\n",
    "\n",
    "finaldata=data[data[\"date\"]==time]\n",
    "finaldata=finaldata[:200]\n",
    "finaldata.shape\n",
    "zeros_df = pd.DataFrame(np.zeros((200, 12)), columns=finaldata.columns)\n",
    "finaldata = pd.concat([finaldata, zeros_df], ignore_index=True)\n",
    "print(finaldata.shape)  # 应该输出 (400, 12)\n",
    "start_date = finaldata['date'].iloc[0]\n",
    "# 生成一个时间序列，从起始日期开始，每秒递增，长度与data相同\n",
    "time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "# 将生成的时间序列赋值回data的'date'列\n",
    "finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "dataset = finaldata\n",
    "print('length of the testing dataset after times : %d', len(dataset))\n",
    "df_stamp = dataset['date']\n",
    "df_stamp.date = pd.to_datetime(df_stamp)\n",
    "\n",
    "\n",
    "\n",
    "data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "data_stamp = torch.FloatTensor(data_stamp)\n",
    "print(data_stamp.shape)\n",
    "\n",
    "dataset = dataset.copy()\n",
    "\n",
    "#dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "dataset = full_scaler.transform(dataset)\n",
    "dataset = torch.FloatTensor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "len(dataset) \n",
    "for x in range(0,1,200):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0267,  0.5010,  0.0000,  0.5010,  0.0590,  0.0000, -0.0099,  0.0000,\n",
       "          0.5005,  0.0000]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[200:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataset in one batch: %d 1\n",
      "==============================\n",
      "number of batches: %d 1\n"
     ]
    }
   ],
   "source": [
    "# 构造样本\n",
    "samples = []\n",
    "Dte_samples=[]\n",
    "count = 0\n",
    "for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "    # train_x, x_mark, train_y, y_mark\n",
    "    s_begin = index\n",
    "    s_end = s_begin + configs.seq_len # 200\n",
    "    r_begin = s_end - configs.label_len # 100\n",
    "    r_end = r_begin + configs.label_len + configs.pred_len #300\n",
    "    seq_x = dataset[s_begin:s_end] # 0-200\n",
    "    seq_y = dataset[r_begin:r_end] # 100-300\n",
    "    seq_x_mark = data_stamp[s_begin:s_end] # 0-200\n",
    "    seq_y_mark = data_stamp[r_begin:r_end] # 100-300\n",
    "    samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print('number of dataset in one batch: %d', count)\n",
    "samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "print('='*30)\n",
    "Dte_samples.append(samples)\n",
    "print('number of batches: %d', len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "Dte_datasets = [CustomDataset(dataloader) for dataloader in Dte_samples]\n",
    "Dte_combined_dataset = ConcatDataset(Dte_datasets)\n",
    "Dte_combined_dataloader = DataLoader(Dte_combined_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from Embed import DataEmbedding\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "# Define your model architecture\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding layer\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=configs.d_model,\n",
    "            hidden_size=configs.d_model,\n",
    "            num_layers=configs.e_layers,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        num_directions = 1\n",
    "        self.projection = nn.Linear(configs.d_model * num_directions, configs.c_out)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask=None):\n",
    "        x_enc = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        lstm_out, _ = self.lstm(x_enc)\n",
    "        lstm_out = self.projection(lstm_out)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return lstm_out[:, -self.pred_len:, :], None\n",
    "        else:\n",
    "            return lstm_out[:, -self.pred_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Informer, Autoformer, Transformer, Reformer,LSTM,GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "def acquire_device():\n",
    "    device = torch.device('cpu')\n",
    "    print('Use CPU')\n",
    "    return device\n",
    "device = acquire_device()\n",
    "\n",
    "model_path = r\"D:\\result\\second\\axis2_LSTM_Custom_axis2_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\checkpoints\\checkpoint.pth\"\n",
    "model = Model(configs).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict( batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -configs.pred_len:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def _run_model():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        return outputs\n",
    "    outputs = _run_model()\n",
    "\n",
    "    f_dim = 0\n",
    "    outputs = outputs[:, -configs.pred_len:, f_dim:]\n",
    "    batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n",
    "\n",
    "    return outputs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n"
     ]
    }
   ],
   "source": [
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        #print('batch_x shape',batch_x.shape)\n",
    "\n",
    "        outputs, batch_y = predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2], dtype=int64)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确保张量在 CPU 上\n",
    "outputs_cpu = outputs[0].cpu()\n",
    "# 转换为 NumPy 数组\n",
    "outputs_numpy = outputs_cpu.detach().numpy()\n",
    "# 打印 NumPy 数组的形状以确认转换\n",
    "print(outputs_numpy.shape)\n",
    "outputs_numpy=full_scaler.inverse_transform(outputs_numpy)\n",
    "\n",
    "res=outputs_numpy#[1,:].reshape(1,10)\n",
    "res.shape\n",
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the testing dataset after times 0.2: %d 8600\n",
      "torch.Size([8600, 6])\n",
      "number of dataset in one batch: %d 42\n",
      "==============================\n",
      "number of batches: %d 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from timefeatures import time_features\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "# load the scaler\n",
    "import pathlib as Path\n",
    "import pickle\n",
    "Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "with open(Scaler_file, 'rb') as f:\n",
    "    full_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "Dte_samples=[]\n",
    "finaldata=data[data[\"date\"]==time]\n",
    "start_date = finaldata['date'].iloc[0]\n",
    "# 生成一个时间序列，从起始日期开始，每秒递增，长度与data相同\n",
    "time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "# 将生成的时间序列赋值回data的'date'列\n",
    "finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "dataset = finaldata\n",
    "print('length of the testing dataset after times 0.2: %d', len(dataset))\n",
    "\n",
    "df_stamp = dataset['date']\n",
    "df_stamp.date = pd.to_datetime(df_stamp)\n",
    "data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "data_stamp = torch.FloatTensor(data_stamp)\n",
    "print(data_stamp.shape)\n",
    "\n",
    "dataset = dataset.copy()\n",
    "\n",
    "#dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "\n",
    "dataset = full_scaler.transform(dataset)\n",
    "dataset = torch.FloatTensor(dataset)\n",
    "# 构造样本\n",
    "samples = []\n",
    "count = 0\n",
    "for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "    # train_x, x_mark, train_y, y_mark\n",
    "    s_begin = index\n",
    "    s_end = s_begin + configs.seq_len\n",
    "    r_begin = s_end - configs.label_len\n",
    "    r_end = r_begin + configs.label_len + configs.pred_len\n",
    "    seq_x = dataset[s_begin:s_end]\n",
    "    seq_y = dataset[r_begin:r_end]\n",
    "    seq_x_mark = data_stamp[s_begin:s_end]\n",
    "    seq_y_mark = data_stamp[r_begin:r_end]\n",
    "    samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "    count += 1\n",
    "\n",
    "\n",
    "print('number of dataset in one batch: %d', count)\n",
    "samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "print('='*30)\n",
    "Dte_samples.append(samples)\n",
    "print('number of batches: %d', len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 10])\n",
      "torch.Size([32, 300, 10])\n",
      "torch.Size([32, 200, 6])\n",
      "torch.Size([32, 300, 6])\n"
     ]
    }
   ],
   "source": [
    "for x,y,z,p in samples:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(z.shape)\n",
    "    print(p.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta\n",
    "# from timefeatures import time_features\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader,IterableDataset,ConcatDataset,TensorDataset\n",
    "\n",
    "# # load the scaler\n",
    "# import pathlib as Path\n",
    "# import pickle\n",
    "# Scaler_file = r'D:\\result\\second\\axis1_LSTM_Custom_axis1_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\data\\scaler.pkl'\n",
    "# with open(Scaler_file, 'rb') as f:\n",
    "#     full_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "# Dte_samples=[]\n",
    "# for time in after:\n",
    "#     finaldata=data[data[\"date\"]==time]\n",
    "#     start_date = finaldata['date'].iloc[0]\n",
    "#     # 生成一个时间序列，从起始日期开始，每秒递增，长度与data相同\n",
    "#     time_series = [start_date + timedelta(seconds=i) for i in range(len(finaldata))]\n",
    "\n",
    "#     # 将生成的时间序列赋值回data的'date'列\n",
    "#     finaldata.loc[:, 'date'] = time_series\n",
    "\n",
    "#     dataset = finaldata\n",
    "#     print('length of the testing dataset after times 0.2: %d', len(dataset))\n",
    "\n",
    "#     df_stamp = dataset['date']\n",
    "#     df_stamp.date = pd.to_datetime(df_stamp)\n",
    "#     data_stamp = time_features(df_stamp, timeenc=1, freq='s')\n",
    "#     data_stamp = torch.FloatTensor(data_stamp)\n",
    "#     print(data_stamp.shape)\n",
    "\n",
    "#     dataset = dataset.copy()\n",
    "\n",
    "#     dataset.drop(['date',\"Schadensklasse\"], axis=1, inplace=True)\n",
    "#     dataset = pd.DataFrame(dataset, columns=finaldata.columns.drop(['date', 'Schadensklasse']))\n",
    "\n",
    "#     dataset = full_scaler.transform(dataset)\n",
    "#     dataset = torch.FloatTensor(dataset)\n",
    "#     # 构造样本\n",
    "#     samples = []\n",
    "#     count = 0\n",
    "#     for index in range(0, len(dataset) - configs.seq_len - configs.pred_len + 1, configs.step_size):\n",
    "#         # train_x, x_mark, train_y, y_mark\n",
    "#         s_begin = index\n",
    "#         s_end = s_begin + configs.seq_len\n",
    "#         r_begin = s_end - configs.label_len\n",
    "#         r_end = r_begin + configs.label_len + configs.pred_len\n",
    "#         seq_x = dataset[s_begin:s_end]\n",
    "#         seq_y = dataset[r_begin:r_end]\n",
    "#         seq_x_mark = data_stamp[s_begin:s_end]\n",
    "#         seq_y_mark = data_stamp[r_begin:r_end]\n",
    "#         samples.append((seq_x, seq_y, seq_x_mark, seq_y_mark))\n",
    "#         count += 1\n",
    "\n",
    "\n",
    "#     print('number of dataset in one batch: %d', count)\n",
    "#     samples = DataLoader(dataset=samples, batch_size=32, shuffle=False, num_workers=0, drop_last=False, pin_memory=True)\n",
    "#     print('='*30)\n",
    "#     Dte_samples.append(samples)\n",
    "# print('number of batches: %d', len(Dte_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "Dte_datasets = [CustomDataset(dataloader) for dataloader in Dte_samples]\n",
    "Dte_combined_dataset = ConcatDataset(Dte_datasets)\n",
    "Dte_combined_dataloader = DataLoader(Dte_combined_dataset, batch_size=1, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n",
      "0\n",
      "torch.Size([1, 200, 10])\n",
      "1\n",
      "torch.Size([1, 200, 10])\n",
      "2\n",
      "torch.Size([1, 200, 10])\n",
      "3\n",
      "torch.Size([1, 200, 10])\n",
      "4\n",
      "torch.Size([1, 200, 10])\n",
      "5\n",
      "torch.Size([1, 200, 10])\n",
      "6\n",
      "torch.Size([1, 200, 10])\n",
      "7\n",
      "torch.Size([1, 200, 10])\n",
      "8\n",
      "torch.Size([1, 200, 10])\n",
      "9\n",
      "torch.Size([1, 200, 10])\n",
      "10\n",
      "torch.Size([1, 200, 10])\n",
      "11\n",
      "torch.Size([1, 200, 10])\n",
      "12\n",
      "torch.Size([1, 200, 10])\n",
      "13\n",
      "torch.Size([1, 200, 10])\n",
      "14\n",
      "torch.Size([1, 200, 10])\n",
      "15\n",
      "torch.Size([1, 200, 10])\n",
      "16\n",
      "torch.Size([1, 200, 10])\n",
      "17\n",
      "torch.Size([1, 200, 10])\n",
      "18\n",
      "torch.Size([1, 200, 10])\n",
      "19\n",
      "torch.Size([1, 200, 10])\n",
      "20\n",
      "torch.Size([1, 200, 10])\n",
      "21\n",
      "torch.Size([1, 200, 10])\n",
      "22\n",
      "torch.Size([1, 200, 10])\n",
      "23\n",
      "torch.Size([1, 200, 10])\n",
      "24\n",
      "torch.Size([1, 200, 10])\n",
      "25\n",
      "torch.Size([1, 200, 10])\n",
      "26\n",
      "torch.Size([1, 200, 10])\n",
      "27\n",
      "torch.Size([1, 200, 10])\n",
      "28\n",
      "torch.Size([1, 200, 10])\n",
      "29\n",
      "torch.Size([1, 200, 10])\n",
      "30\n",
      "torch.Size([1, 200, 10])\n",
      "31\n",
      "torch.Size([1, 200, 10])\n",
      "32\n",
      "torch.Size([1, 200, 10])\n",
      "33\n",
      "torch.Size([1, 200, 10])\n",
      "34\n",
      "torch.Size([1, 200, 10])\n",
      "35\n",
      "torch.Size([1, 200, 10])\n",
      "36\n",
      "torch.Size([1, 200, 10])\n",
      "37\n",
      "torch.Size([1, 200, 10])\n",
      "38\n",
      "torch.Size([1, 200, 10])\n",
      "39\n",
      "torch.Size([1, 200, 10])\n",
      "40\n",
      "torch.Size([1, 200, 10])\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "    print(batch_x.shape)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from Embed import DataEmbedding\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "# Define your model architecture\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding layer\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=configs.d_model,\n",
    "            hidden_size=configs.d_model,\n",
    "            num_layers=configs.e_layers,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer\n",
    "        num_directions = 1\n",
    "        self.projection = nn.Linear(configs.d_model * num_directions, configs.c_out)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask=None):\n",
    "        x_enc = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        lstm_out, _ = self.lstm(x_enc)\n",
    "        lstm_out = self.projection(lstm_out)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return lstm_out[:, -self.pred_len:, :], None\n",
    "        else:\n",
    "            return lstm_out[:, -self.pred_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "def acquire_device():\n",
    "    device = torch.device('cpu')\n",
    "    print('Use CPU')\n",
    "    return device\n",
    "device = acquire_device()\n",
    "import torch\n",
    "\n",
    "model_path = r\"D:\\result\\second\\axis2_LSTM_Custom_axis2_ftM_sl200_ll100_pl200_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0\\checkpoints\\checkpoint.pth\"\n",
    "model = Model(configs).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict( batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -200:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :100, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def _run_model():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        return outputs\n",
    "    outputs = _run_model()\n",
    "\n",
    "    f_dim = 0\n",
    "    outputs = outputs[:, -200:, f_dim:]\n",
    "    batch_y = batch_y[:, -200:, f_dim:].to(device)\n",
    "\n",
    "    return outputs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n",
      "torch.Size([1, 200, 10])\n"
     ]
    }
   ],
   "source": [
    "for i,(batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(Dte_combined_dataloader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        #print('batch_x shape',batch_x.shape)\n",
    "\n",
    "        outputs, batch_y = predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "        print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 10])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "# 确保张量在 CPU 上\n",
    "outputs_cpu = outputs[0].cpu()\n",
    "# 转换为 NumPy 数组\n",
    "outputs_numpy = outputs_cpu.detach().numpy()\n",
    "# 打印 NumPy 数组的形状以确认转换\n",
    "print(outputs_numpy.shape)\n",
    "outputs_numpy=full_scaler.inverse_transform(outputs_numpy)\n",
    "\n",
    "res=outputs_numpy[1,:].reshape(1,10)\n",
    "res.shape\n",
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=outputs_numpy[1,:].reshape(1,10)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the axis1 model with 10 best features\n",
    "model = load('10_best_xgboost_modeltestaxis1.joblib')\n",
    "res=model.predict(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
